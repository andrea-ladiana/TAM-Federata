# ---- __init__.py ----
# unsup package: exports core modules\nfrom .networks import *  # noqa\nfrom .dynamics import *  # noqa\nfrom .dynamics_single_mode import *  # noqa\nfrom .functions import *  # noqa\n


# ---- config.py ----
# -*- coding: utf-8 -*-
"""
Configurazione e iperparametri (single mode only).

Questo modulo centralizza gli iperparametri usati negli esperimenti in modalità
*single* (nessuna accumulazione dei dati tra i round, a parte la memoria Ebraica
da round t-1 per il blending).

Note:
- Le soglie e la pipeline sono allineate alle specifiche del report (τ, ρ, qthr,
  blending, propagazione pseudo-inversa, ecc.).
- Il modulo espone una dataclass principale `HyperParams` con metodi di utilità.
"""
from __future__ import annotations

from dataclasses import dataclass, field
from math import ceil
from typing import Literal, Optional


Mode = Literal["single"]  # hard-lock alla single-mode


@dataclass(frozen=True)
class TAMParams:
    """Parametri della dinamica TAM."""
    beta_T: float = 2.5
    lam: float = 0.2
    h_in: float = 0.1
    updates: int = 50
    # noise/annealing: i default devono combaciare con TAM_Network
    noise_scale: float = 0.3
    min_scale: float = 0.02
    anneal: bool = True
    schedule: Literal["linear", "exp"] = "linear"


@dataclass(frozen=True)
class PropagationParams:
    """Parametri per la propagazione pseudo-inversa J -> J_KS."""
    iters: int = 200
    eps: float = 1e-2  # usato internamente da `propagate_J` (se rilevante)


@dataclass(frozen=True)
class SpectralParams:
    """Soglie per inizializzazione/pruning nel disentangling."""
    tau: float = 0.5     # cut sugli autovalori di J_KS
    rho: float = 0.6     # allineamento spettrale ξᵀ J_KS ξ / N
    qthr: float = 0.4    # pruning per overlap mutuo tra candidati


@dataclass
class HyperParams:
    # --- scala problema / dataset federato ---
    L: int = 3           # numero client/layer
    K: int = 3           # numero archetipi
    N: int = 300         # dimensione dei pattern
    n_batch: int = 24    # round
    M_total: int = 2400  # numero totale esempi
    r_ex: float = 0.8    # correlazione media campioni/archetipo
    K_per_client: Optional[int] = None  # se None -> ceil(K / L)

    # --- blending e stima single-round ---
    w: float = 0.4  # peso dell'unsupervised vs memoria ebraica del round precedente

    # --- modalità (hard-lock) ---
    mode: Mode = "single"

    # --- seed/repliche ---
    n_seeds: int = 5
    seed_base: int = 1234
    use_tqdm: bool = True
    # Numero di worker per esecuzione parallela su seed (1 = sequenziale)
    n_workers: int = 6

    # --- sottostrutture ---
    tam: TAMParams = field(default_factory=TAMParams)
    prop: PropagationParams = field(default_factory=PropagationParams)
    spec: SpectralParams = field(default_factory=SpectralParams)

    # --- varie utilità/flags ---
    estimate_keff_method: Literal["shuffle", "mp"] = "shuffle"
    ema_alpha: float = 0.0  # 0.0 = off (EMA su J_unsup se si vuole smussare il rumore)

    def __post_init__(self):
        # lock single-mode
        if self.mode != "single":
            raise ValueError("Questo pacchetto è bloccato alla modalità 'single'.")
        if not (0.0 <= self.w <= 1.0):
            raise ValueError("w dev'essere in [0, 1].")
        if not (0 < self.r_ex <= 1.0):
            raise ValueError("r_ex dev'essere in (0, 1].")
        if self.K_per_client is None:
            object.__setattr__(self, "K_per_client", max(1, ceil(self.K / self.L)))

    # -------- helpers pratici --------
    @property
    def M_per_client_per_round(self) -> int:
        """Numero di esempi per *client* per *round* in single-mode."""
        return max(1, ceil(self.M_total / (self.L * self.n_batch)))

    @property
    def M_eff_round(self) -> int:
        """Effettivo numero di esempi per stima J in *single* (per client)."""
        # Possibile diversa normalizzazione a seconda della tua implementazione di unsupervised_J;
        # qui esponiamo un valore coerente con "single".
        return self.M_per_client_per_round

    def copy_with(self, **kwargs) -> "HyperParams":
        """Clona con override di alcuni campi (comodo in sweep/ablazioni)."""
        data = {**self.__dict__}
        data.update(kwargs)
        return HyperParams(**data)


# ---- data.py ----
# src/unsup/data.py
from __future__ import annotations
import math
from typing import List, Tuple, Optional, Sequence, Dict

import numpy as np

# Reuse generator for true patterns from the existing functions module
# (avoid duplications and keep one single source of truth)
from .functions import gen_patterns as _gen_patterns  # noqa: F401


__all__ = [
    "make_client_subsets",
    "gen_dataset_partial_archetypes",
    "new_round_single",
    "compute_round_coverage",
    "count_exposures",
]


def make_client_subsets(
    K: int,
    L: int,
    K_per_client: int,
    rng: np.random.Generator,
) -> List[List[int]]:
    """
    Crea i sottoinsiemi di archetipi per ciascun client garantendo (quando fattibile)
    che l'unione copra tutti gli archetipi {0,...,K-1}.

    Vincolo di fattibilità: L * K_per_client >= K (altrimenti non è possibile coprire tutti).

    Strategia:
      1) Round-robin su una permutazione casuale per garantire copertura.
      2) Riempimento fino a K_per_client per ciascun client con scelte senza rimpiazzo.

    Returns
    -------
    subsets : list of list[int]
        Lista di lunghezza L; ogni entry è l'insieme (ordinato) di archetipi assegnati.
    """
    if K <= 0 or L <= 0:
        raise ValueError("K e L devono essere positivi.")
    if K_per_client <= 0:
        raise ValueError("K_per_client deve essere > 0.")
    if K_per_client > K:
        raise ValueError("K_per_client non può superare K.")
    if L * K_per_client < K:
        raise ValueError(
            f"Impossibile coprire {K} archetipi con L={L} e K_per_client={K_per_client} "
            f"(L*K_per_client={L*K_per_client} < K)."
        )

    subsets: List[List[int]] = [[] for _ in range(L)]
    perm = rng.permutation(K)

    # Passo 1: round-robin per la copertura
    for i, mu in enumerate(perm):
        subsets[i % L].append(int(mu))

    # Passo 2: riempimento fino a K_per_client con scelte senza rimpiazzo locali
    for l in range(L):
        need = K_per_client - len(subsets[l])
        if need <= 0:
            continue
        pool = [mu for mu in range(K) if mu not in subsets[l]]
        if need > len(pool):
            # Se qui capitasse, significa che K_per_client è molto grande e gli altri client
            # hanno già “assorbito” quasi tutto: in tal caso consenti duplicati controllati.
            extra = rng.choice(range(K), size=need, replace=False)
        else:
            extra = rng.choice(pool, size=need, replace=False)
        subsets[l].extend(int(x) for x in extra)

    # Normalizza (ordina) e deduplica
    return [sorted(set(s)) for s in subsets]


def gen_dataset_partial_archetypes(
    xi_true: np.ndarray,
    M_total: int,
    r_ex: float,
    n_batch: int,
    L: int,
    client_subsets: Sequence[Sequence[int]],
    rng: np.random.Generator,
    use_tqdm: bool = False,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Genera dataset UNSUPERVISED in SINGLE-mode con copertura parziale per client.

    Ogni client l ha un sottoinsieme consentito di archetipi (client_subsets[l]).
    Per ogni round t, si generano M_c esempi campionando archetipi solo dal sottoinsieme:
        eta[l, t, m] = chi * xi_true[mu], chi ∈ {±1}^N, con P(chi_i = +1) = (1 + r_ex)/2.

    Parametri
    ---------
    xi_true : (K, N)
        Archetipi veri (±1).
    M_total : int
        Numero desiderato di esempi totali (su TUTTI i client e TUTTI i round).
        Ogni client e round riceve M_c = ceil(M_total / (L * n_batch)) esempi.
    r_ex : float in [0, 1]
        Livello medio di qualità/correlazione degli esempi.
    n_batch : int
        Numero di round.
    L : int
        Numero di client.
    client_subsets : sequence of sequences
        Liste degli archetipi consentiti per client (indici in [0, K-1]).
    rng : np.random.Generator
        Generatore di numeri casuali.
    use_tqdm : bool
        Se True, mostra barre di avanzamento (se disponibile).

    Returns
    -------
    ETA : (L, n_batch, M_c, N) float32
        Esempi generati per client/round.
    labels : (L, n_batch, M_c) int32
        Indice di archetipo usato per ciascun esempio (per comodità/metriche).
    """
    K, N = xi_true.shape
    if len(client_subsets) != L:
        raise ValueError("client_subsets deve avere lunghezza L.")

    M_c = int(math.ceil(M_total / float(L * n_batch)))
    ETA = np.zeros((L, n_batch, M_c, N), dtype=np.float32)
    labels = np.zeros((L, n_batch, M_c), dtype=np.int32)

    p_keep = 0.5 * (1.0 + float(r_ex))
    itL = range(L)

    # TQDM opzionale, senza dipendenza hard
    if use_tqdm:
        try:
            from tqdm import tqdm as _tqdm  # type: ignore
            itL = _tqdm(itL, desc="dataset: clients", leave=False)
        except Exception:
            pass

    for l in itL:
        allowed = list(client_subsets[l])
        if not allowed:
            raise ValueError(f"Client {l} ha subset vuoto.")
        itT = range(n_batch)
        if use_tqdm:
            try:
                from tqdm import tqdm as _tqdm  # type: ignore
                itT = _tqdm(itT, desc=f"client {l} rounds", leave=False)
            except Exception:
                pass

        for t in itT:
            mus = rng.choice(allowed, size=M_c, replace=True).astype(int)  # (M_c,)
            probs = rng.uniform(size=(M_c, N))
            chi = np.where(probs <= p_keep, 1.0, -1.0).astype(np.float32)  # (M_c, N)
            xi_sel = xi_true[mus].astype(np.float32)                         # (M_c, N)
            ETA[l, t] = (chi * xi_sel).astype(np.float32)
            labels[l, t] = mus.astype(np.int32)

    return ETA, labels


def new_round_single(ETA: np.ndarray, t: int) -> np.ndarray:
    """
    Estrae il tensore del SOLO round t: (L, M_c, N) da un ETA (L, n_batch, M_c, N).
    """
    if ETA.ndim != 4:
        raise ValueError("ETA deve avere shape (L, n_batch, M_c, N).")
    return np.asarray(ETA[:, t, :, :], dtype=np.float32)


def compute_round_coverage(labels_t: np.ndarray, K: int) -> float:
    """
    Coverage round-wise: frazione di archetipi distinti osservati al round t.

    Parametri
    ---------
    labels_t : (L, M_c) oppure (M_c,)
        Etichette (indici archetipi) del solo round t (eventualmente già aggregate sui client).
    K : int
        Numero totale di archetipi.

    Returns
    -------
    cov : float in [0,1]
    """
    if labels_t.ndim == 1:
        seen = set(int(mu) for mu in labels_t)
    elif labels_t.ndim == 2:
        L, M_c = labels_t.shape
        seen = set(int(mu) for l in range(L) for mu in labels_t[l])
    else:
        raise ValueError("labels_t deve avere ndim 1 o 2.")
    return len(seen) / float(K)


def count_exposures(labels: np.ndarray, K: int) -> np.ndarray:
    """
    Conta quante volte ciascun archetipo è stato “visto” in tutto il dataset.
    Utile per correlare “esposizione” e magnetizzazione nel test Hopfield post-hoc.

    Parametri
    ---------
    labels : (L, n_batch, M_c)
    K : int

    Returns
    -------
    exposure : (K,) np.ndarray di int
    """
    if labels.ndim != 3:
        raise ValueError("labels deve avere shape (L, n_batch, M_c).")
    expo = np.zeros((K,), dtype=np.int64)
    flat = labels.reshape(-1)
    vals, cnts = np.unique(flat, return_counts=True)
    expo[vals.astype(int)] = cnts.astype(int)
    return expo


# ---- dynamics.py ----
# -*- coding: utf-8 -*-
"""
Dinamiche e disentangling (single mode).

Espone:
- `new_round`: estrazione batch round-t in single-mode.
- `eigen_cut`: selezione autovettori informativi da J_KS.
- `init_candidates_from_eigs`: generazione σ(0) via mixture di segni.
- `disentangling`: dinamica TAM + pruning + magnetizzazioni.
- `dis_check`: wrapper che garantisce almeno K candidati (con fallback).

Dipendenze chiave:
- src.unsup.networks.TAM_Network
- src.unsup.functions.propagate_J (a monte, non qui)
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, Tuple
import numpy as np

# Import locali (nessuna dipendenza da TF)
from src.unsup.networks import TAM_Network
from src.unsup.config import TAMParams, SpectralParams


__all__ = [
    "new_round",
    "eigen_cut",
    "init_candidates_from_eigs",
    "disentangling",
    "dis_check",
]


def new_round(ETA: np.ndarray, t: int) -> np.ndarray:
    """
    Estrae il batch del *round t* in modalità single.

    Parameters
    ----------
    ETA : np.ndarray
        Tensore federato shape (L, T, M_c, N).
    t : int
        Indice round (0..T-1).

    Returns
    -------
    np.ndarray
        Batch shape (L, M_c, N) per il round t.
    """
    if ETA.ndim != 4:
        raise ValueError(f"ETA atteso di rank 4 (L,T,M_c,N); ricevuto shape={ETA.shape}")
    L, T, Mc, N = ETA.shape
    if not (0 <= t < T):
        raise IndexError(f"round t={t} fuori range [0, {T-1}]")
    return ETA[:, t, :, :]


def eigen_cut(JKS: np.ndarray, tau: float = 0.5) -> Tuple[np.ndarray, np.ndarray]:
    """
    Taglio spettrale su J_KS: seleziona autovettori con autovalori > tau.

    Returns
    -------
    (vals_sel, vecs_sel)
      vals_sel : (K_eff,)
      vecs_sel : (K_eff, N)  # righe = autovettori trasposti per coerenza con generazione
    """
    if JKS.ndim != 2 or JKS.shape[0] != JKS.shape[1]:
        raise ValueError("JKS deve essere quadrata.")
    # Use symmetric eigendecomposition for speed and stability
    vals, vecs = np.linalg.eigh(JKS)
    mask = vals > float(tau)
    V = vecs[:, mask].T  # (K_eff, N)
    return vals[mask], V


def init_candidates_from_eigs(V: np.ndarray, L: int, s: int | None = None, rng: np.random.Generator | None = None) -> np.ndarray:
    """
    Genera stati iniziali σ(0) combinando gli autovettori informativi con
    pesi gaussiani e prendendo il segno.

    Parameters
    ----------
    V : np.ndarray
        Autovettori selezionati, shape (K_eff, N).
    L : int
        Numero layer/client (serve a tarare il numero di mix).
    s : int | None
        Numero di combinazioni. Di default segue la formula empirica del notebook:
        s = 10 * int(K_eff / L * log(K_eff / 0.01)).
    rng : np.random.Generator | None
        RNG opzionale per riproducibilità.

    Returns
    -------
    np.ndarray
        σ(0) shape (s, N), valori in {-1, +1}.
    """
    if V.ndim != 2:
        raise ValueError("V deve avere shape (K_eff, N).")
    K_eff, N = V.shape
    if K_eff == 0:
        # fallback: nessun autovettore informativo
        return np.empty((0, N), dtype=int)

    if s is None:
        s = max(1, 10 * int((K_eff / max(1, L)) * np.log(K_eff / 0.01)))

    rng = np.random.default_rng() if rng is None else rng
    W = rng.normal(0.0, 1.0, size=(s, K_eff))  # (s, K_eff)
    # Correct multiplication: mix weights W (s, K_eff) with eigenvectors V (K_eff, N)
    # to obtain Z of shape (s, N). Previous code used (V @ W.T).T which requires
    # N == K_eff and caused a ValueError when they differ.
    Z = W @ V  # (s, N)
    sigma0 = np.where(Z >= 0.0, 1, -1).astype(int)
    return sigma0


@dataclass
class _DisResult:
    xi_r: np.ndarray      # (K_tilde, N)
    magnetisations: np.ndarray  # (K_tilde,)


def _prune_and_score(xi_r: np.ndarray, JKS: np.ndarray, xi_true: np.ndarray, spec: SpectralParams) -> _DisResult:
    """
    Applica:
      1) allineamento spettrale: <ξ, JKS ξ>/N >= rho
      2) pruning per overlap mutuo: |<ξ_a, ξ_b>|/N <= qthr
    e calcola magnetizzazioni: m_a = max_b |<ξ_a, ξ_b* >|/N
    """
    if xi_r.size == 0:
        return _DisResult(xi_r=np.empty((0, JKS.shape[0]), dtype=int), magnetisations=np.array([]))

    N = JKS.shape[0]
    # 1) spectral alignment
    # quad[a,i] = sum_j JKS[i,j] * xi_r[a,j] = (xi_r @ JKS^T)[a,i]
    quad = (xi_r @ JKS.T)
    align = (np.einsum("ai,ai->a", xi_r, quad) / N)   # (K_tilde,)
    keep = align >= float(spec.rho)
    xi_r = xi_r[keep]
    if xi_r.size == 0:
        return _DisResult(xi_r=np.empty((0, N), dtype=int), magnetisations=np.array([]))

    # 2) mutual-overlap pruning (sopra diagonale)
    q = np.abs((xi_r @ xi_r.T) / N).astype(float)
    iu = np.triu_indices(q.shape[0], k=1)
    bad_pairs = q[iu] > float(spec.qthr)
    if np.any(bad_pairs):
        # Strategia semplice: rimuovi gli indici ripetuti (first-occur policy).
        A = q.copy()
        to_remove: set[int] = set()
        for i, j in zip(*iu):
            if A[i, j] > spec.qthr:
                # rimuovi j (arbitrario/greedy)
                to_remove.add(j)
        if to_remove:
            mask = np.ones(xi_r.shape[0], dtype=bool)
            mask[list(sorted(to_remove))] = False
            xi_r = xi_r[mask]

    # magnetizzazioni (max overlap con veri archetipi)
    M = np.abs(xi_r @ xi_true.T) / N  # (K_tilde, K)
    m = M.max(axis=1) if M.size else np.array([])
    return _DisResult(xi_r=xi_r, magnetisations=m)


def disentangling(
    V: np.ndarray,
    K: int,
    L: int,
    J_rec: np.ndarray,
    JKS_iter: np.ndarray,
    xi_true: np.ndarray,
    tam: TAMParams,
    spec: SpectralParams,
    xi_prev: np.ndarray | int = -10,
    show_progress: bool = False,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Esegue: init da autovettori -> dinamica TAM -> pruning -> magnetizzazioni.

    Parameters
    ----------
    V : (K_eff, N) autovettori selezionati da `eigen_cut`.
    J_rec : matrice di ricostruzione pre-propagazione (per `prepare` della TAM).
    JKS_iter : matrice J dopo propagazione (per check spettrale).
    xi_prev : se passato (array) concatena le ricostruzioni precedenti (fallback loop).

    Returns
    -------
    (xi_r, m)
      xi_r : (K_tilde, N) candidati finali
      m    : (K_tilde,) magnetizzazioni (max overlap vs archetipi veri)
    """
    N = J_rec.shape[0]
    sigma0 = init_candidates_from_eigs(V, L=L)
    if sigma0.size == 0:
        return np.empty((0, N), dtype=int), np.array([])

    # Replicazione sui layer e preparazione input TAM
    # σ shape attesa da TAM_Network: (s, L, N)
    sigma = np.repeat(sigma0[:, None, :], L, axis=1)

    net = TAM_Network()
    net.prepare(J_rec, L)
    net.dynamics(
        sigma,
        tam.beta_T,
        tam.lam,
        tam.h_in,
        updates=tam.updates,
        noise_scale=tam.noise_scale,
        min_scale=tam.min_scale,
        anneal=tam.anneal,
        schedule=tam.schedule,
        show_progress=show_progress,
        desc="TAM (single)",
    )

    if net.σ is None:
        return np.empty((0, N), dtype=int), np.array([])

    xi_new = np.reshape(np.asarray(net.σ), (sigma0.shape[0] * L, N)).astype(int)

    if isinstance(xi_prev, np.ndarray) and np.mean(xi_prev) != -10:
        xi_all = np.vstack([xi_new, xi_prev])
    else:
        xi_all = xi_new

    # Pruning + magnetizzazioni
    res = _prune_and_score(xi_all, JKS_iter, xi_true, spec)

    # Fallback se tutto rimosso: conserva primi max(1, min(K, K̃))
    if res.xi_r.shape[0] == 0:
        keep = max(1, min(K, xi_all.shape[0]))
        xi_fbk = xi_all[:keep]
        res = _prune_and_score(xi_fbk, JKS_iter, xi_true, spec)

    return res.xi_r, res.magnetisations


def dis_check(
    V: np.ndarray,
    K: int,
    L: int,
    J_rec: np.ndarray,
    JKS_iter: np.ndarray,
    xi_true: np.ndarray,
    tam: TAMParams,
    spec: SpectralParams,
    show_progress: bool = False,
    max_attempts: int = 10,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Wrapper robusto: ripete il disentangling concatenando le soluzioni finché non
    ottiene almeno K candidati (oppure esaurisce i tentativi).

    Returns
    -------
    (xi_r, m)
    """
    xi_r, m = disentangling(
        V, K, L, J_rec, JKS_iter, xi_true, tam=tam, spec=spec, xi_prev=-10, show_progress=show_progress
    )
    attempts = 0
    while xi_r.shape[0] < K and attempts < max_attempts:
        attempts += 1
        xi_r, m = disentangling(
            V, K, L, J_rec, JKS_iter, xi_true, tam=tam, spec=spec, xi_prev=xi_r, show_progress=show_progress
        )
    return xi_r, m


# ---- estimators.py ----
# src/unsup/estimators.py
from __future__ import annotations
from typing import Tuple, Optional

import numpy as np

from .functions import unsupervised_J, Hebb_J  # reuse existing stable primitives


__all__ = [
    "build_unsup_J_single",
    "blend_with_memory",
]


def _symmetrize(J: np.ndarray) -> np.ndarray:
    """Rende J esattamente simmetrica (tollerante a lievi asimmetrie numeriche)."""
    J = np.asarray(J, dtype=np.float32)
    return 0.5 * (J + J.T)


def build_unsup_J_single(ETA_t: np.ndarray, K: int) -> Tuple[np.ndarray, int]:
    """
    Stima J_unsup per il SOLO round t, mediando su layer/client.

    Parametri
    ---------
    ETA_t : (L, M_c, N)
        Esempi del round corrente (single-mode).
    K : int
        Numero di archetipi (usato per definire M_eff = max(1, M_c // K)).

    Returns
    -------
    J_unsup : (N, N) float32
        Media delle matrici unsupervised per layer.
    M_eff : int
        Effective sample size per-layer usato in unsupervised_J (per coerenza con stima K_eff MP).
    """
    if ETA_t.ndim != 3:
        raise ValueError("ETA_t deve avere shape (L, M_c, N).")
    L, M_c, N = ETA_t.shape
    if K <= 0:
        raise ValueError("K deve essere > 0.")
    # Numero effettivo di campioni PER LAYER usati per costruire J in questo round.
    # Questo è il denominatore corretto per unsupervised_J e quello da fornire al metodo MP.
    M_eff = int(M_c)

    # Calcola J_unsup per ciascun layer e media (scalando con M_c corretto)
    Js = [unsupervised_J(np.asarray(ETA_t[l], dtype=np.float32), M_eff) for l in range(L)]
    J_unsup = np.sum(Js, axis=0) / float(L)

    return _symmetrize(J_unsup), M_eff


def blend_with_memory(
    J_unsup: np.ndarray,
    xi_prev: Optional[np.ndarray],
    w: float,
) -> np.ndarray:
    """
    Fonde la stima unsupervised del round corrente con la memoria Hebb del round precedente.

    Regole:
      - Se xi_prev è None -> ritorna J_unsup (primo round).
      - Altrimenti: J_rec = w * J_unsup + (1-w) * J_hebb_prev, con J_hebb_prev = Hebb_J(xi_prev).

    Parametri
    ---------
    J_unsup : (N, N)
        Stima appena ottenuta dal round corrente.
    xi_prev : (S, N) oppure None
        Pattern disentangled ottenuti fino al round precedente (S può essere ≠ K).
    w : float in [0, 1]
        Peso di blending (w alto = più fiducia nel dato corrente).

    Returns
    -------
    J_rec : (N, N)
        Matrice ricostruita post-blend, simmetrizzata.
    """
    J_unsup = _symmetrize(J_unsup)
    if xi_prev is None:
        return J_unsup

    xi_prev = np.asarray(xi_prev, dtype=np.float32)
    if xi_prev.ndim != 2:
        raise ValueError("xi_prev deve avere shape (S, N).")

    J_hebb_prev = Hebb_J(xi_prev)  # equivalente a unsupervised_J(xi_prev, M=1) per binario ±1
    J_rec = float(w) * J_unsup + float(1.0 - w) * J_hebb_prev
    return _symmetrize(J_rec)


# ---- functions.py ----
import numpy as np
from tqdm import tqdm

# Lightweight NumPy backend to avoid TensorFlow/PyTorch usage
class _NPBackend:
    # Provide dtypes for compatibility
    float32 = np.float32

    @staticmethod
    def einsum(subscripts, *operands, **kwargs):
        # Fast-path common contractions used throughout the codebase
        try:
            if subscripts == 'ai,aj->ij' and len(operands) == 2:
                A, B = operands
                return A.T @ B
            if subscripts == 'ij,Aj->Ai' and len(operands) == 2:
                J, S = operands
                return S @ J.T
            if subscripts == 'ki,pi->kp' and len(operands) == 2:
                A, B = operands
                return A @ B.T
            if subscripts == 'ki,pi->pk' and len(operands) == 2:
                A, B = operands
                return B @ A.T
            if subscripts == 'ki,kp,pj->ij' and len(operands) == 3:
                A, C, B = operands
                return (A.T @ C) @ B
            if subscripts == 'ij,slj->sli' and len(operands) == 2:
                J, S = operands
                return np.tensordot(S, J, axes=([2], [1]))
        except Exception:
            pass
        # Default to optimized contraction planning to accelerate einsum-heavy paths
        if 'optimize' not in kwargs:
            kwargs['optimize'] = True
        return np.einsum(subscripts, *operands, **kwargs)

    @staticmethod
    def sign(x):
        # Map zeros to +1 to stay consistent with binary {+1,-1}
        x = np.sign(x)
        return np.where(x >= 0, 1, -1)

    @staticmethod
    def convert_to_tensor(x, dtype=None):
        return np.array(x, dtype=dtype if dtype is not None else None)

    @staticmethod
    def transpose(x, perm=None):
        return np.transpose(x, axes=perm)

    @staticmethod
    def expand_dims(a, axis):
        return np.expand_dims(a, axis=axis)

    @staticmethod
    def reshape(a, shape):
        return np.reshape(a, shape)

tf = _NPBackend()


########################################################
################### HOPFIELD ###########################
########################################################


def gen_patterns(N,P):
    ξ = 2*np.random.randint(0,2,(P,N))-1
    return tf.convert_to_tensor(ξ, dtype=np.float32)

def gen_archetypes(ξ, M, r):
    η = []
    P = np.shape(ξ)[0]; N = np.shape(ξ)[1]
    for μ in range(P):
        probs = np.random.uniform(size=(M,N))
        χ = np.sign(0.5*(1+r)-probs)
        ημ = np.array(tf.einsum('ai,i->ai', χ, ξ[μ]))
        ημ = np.mean(ημ, axis=0)
        η.append(ημ)
    η = tf.convert_to_tensor(η, dtype=np.float32)
    return η

def Hebb_J(η):
    N = np.shape(η)[1]
    return tf.einsum('ai,aj->ij', η, η)/N


########################################################
################# PSEUDO INV ###########################
########################################################

def propagate_J(J, J_real=-1, verbose=True, stop_t=0, iters=20, max_steps: int | None = None, eps: float | None = None, tol: float = 1e-8):
    """Iterative propagation (pseudo-inverse like refinement).

    Notes
    -----
    - `iters` is treated as the (maximum) number of iterations.
    - `eps` controls the step-size; if not provided we default to 1e-2 which matches
      the `PropagationParams.eps` default in the config.
    - The old behaviour divided `iters` by a small eps which could produce very
      large numbers of micro-steps; that produced extremely long runs. This
      implementation uses `iters` directly and a safe default for `eps`.
    """
    # choose sensible defaults
    local_eps = eps if eps is not None else 1e-2

    if stop_t > 0:
        # legacy: translate continuous time stop_t into micro-steps if requested
        nupdates = int(stop_t / local_eps)
    else:
        # Treat `iters` as the number of iterations (safe and predictable).
        nupdates = int(max(1, iters))

    # hard cap to avoid pathological long runs
    HARD_CAP = 20000
    nupdates = min(nupdates, HARD_CAP)
    if max_steps is not None:
        nupdates = min(nupdates, max_steps)

    # use float64 internally for numerical stability
    Jf = np.array(J, dtype=np.float64, copy=True)
    disable = not verbose

    for l in tqdm(range(0, nupdates + 1), disable=disable):
        # step-size schedule similar to original implementation
        step = local_eps / (1 + local_eps * l)

        # Use fast matmul instead of einsum for performance. Guard against
        # numerical blow-up by backing off the step-size up to a few times if
        # the tentative update produces non-finite or excessively large values.
        attempts = 0
        max_backoffs = 6
        cur_step = step
        while True:
            update = cur_step * (Jf - (Jf @ Jf))
            Jf_new = Jf + update
            # check for numerical issues or runaway norms
            if np.isfinite(Jf_new).all() and np.linalg.norm(Jf_new, ord='fro') < 1e12:
                Jf = Jf_new
                break
            else:
                attempts += 1
                cur_step = cur_step * 0.5
                if attempts >= max_backoffs or cur_step < 1e-16:
                    # give up this update and zero it to avoid NaNs
                    update = np.zeros_like(Jf)
                    break

        # optional progress/logging
        if np.mean(J_real) != -1 and l % 1000 == 0:
            try:
                print(np.linalg.norm(J_real - Jf, ord='fro'))
            except Exception:
                pass

        # stop on NaNs/Infs (numerical blow-up) -- should be prevented above
        if not np.isfinite(Jf).all():
            Jf[np.logical_not(np.isfinite(Jf))] = 0.0
            break

        # relative early stopping: update small relative to matrix norm
        denom = np.linalg.norm(Jf, ord='fro')
        if denom <= 1e-12:
            denom = 1.0
        if np.linalg.norm(update, ord='fro') / denom < tol:
            break

    # cast back to original dtype if input was float32
    return Jf.astype(J.dtype) if hasattr(J, 'dtype') else Jf


def dreaming_t(ξ, t):
    N = ξ.shape[1]
    C = tf.einsum('ki,pi->kp', ξ, ξ)/N
    kernel = (1 + t * C)/(1+t)
    C_inv = np.linalg.inv(kernel)
    J = tf.einsum('ki,kp,pj->ij', ξ, C_inv, ξ)/N
    return J

def dreaming_t_unsup(ξ, t, M):
    N = ξ.shape[1]
    C = tf.einsum('ki,pi->kp', ξ, ξ)/(M*N)
    kernel = (1 + t * C)
    C_inv = np.linalg.inv(kernel)
    J = tf.einsum('ki,kp,pj->ij', ξ, C_inv*(1+t), ξ)/(M*N)
    return J


def JK_real(ξ):
    N = ξ.shape[1]
    C = tf.einsum('ki,pi->kp', ξ, ξ)/N
    C_inv = np.linalg.inv(C)
    J = tf.einsum('ki,kp,pj->ij', ξ, C_inv, ξ)/N
    return J



########################################################
################# EXAMPLES #############################
########################################################
def gen_dataset_unsup(ξ, M, r, n_split, L, legacy: bool = False):
    """Generate an unsupervised (no labels) dataset split across L clients and n_split rounds.

    Parameters
    ----------
    ξ : array (K, N)
        True archetypes/patterns.
    M : int
        TOTAL number of examples you want (across all clients and all rounds) if legacy=False.
        (Legacy behaviour: interpreted roughly as examples per round, NOT divided by L or K.)
    r : float in [0,1]
        Average quality / correlation parameter.
    n_split : int
        Number of rounds (batches) per client.
    L : int
        Number of clients.
    legacy : bool (default False)
        If True, keep the original (inflated) allocation used previously.

    Returns
    -------
    η : tensor (L, n_split, M_per_client_round, N)
        For each client l and round m we provide a set of examples with shape (M_per_client_round, N).

    Notes
    -----
    New (default) semantics aim so that the *total* number of examples over all clients and rounds
    is close to M (can be slightly >= due to ceil). Old code allocated ~K times more per client/round.
    Set legacy=True to reproduce previous behaviour for comparison.
    """
    η = []
    P = np.shape(ξ)[0]; N = np.shape(ξ)[1]

    if legacy:
        # Original (inflated) behaviour: each client & round gets (M//n_split)*P examples
        num_per_example = int(M // n_split)
    else:
        # Target: total examples ≈ M = L * n_split * (num_per_example * P)
        # => num_per_example ≈ M / (L * n_split * P)
        num_per_example = int(np.ceil(M / (L * n_split * P)))
        if num_per_example < 1:
            num_per_example = 1

    for μ in range(P):
        probs = np.random.uniform(size=(L, n_split, num_per_example, N))
        χ = np.sign(0.5*(1+r)-probs)
        ημ = np.array(tf.einsum('akmi,i->mkai', χ, ξ[μ]))  # (num_per_example, n_split, L, N)
        η.append(ημ)

    η = np.reshape(np.array(η), (num_per_example * P, n_split, L, N))  # (num_per_example*P, n_split, L, N)
    # Reorder to (L, n_split, num_per_example*P, N) (same layout as before so downstream code still works)
    η = np.einsum('lmAi,Amli->lmAi', np.ones((L, n_split, num_per_example * P, N)), η)
    η = tf.convert_to_tensor(η, dtype=np.float32)

    if not legacy:
        # Lightweight sanity info (can be commented out if too verbose)
        total_alloc = L * n_split * num_per_example * P
        if total_alloc != M:
            # Silent mismatch is okay; just informative.
            pass  # (leave a placeholder, no print to avoid notebook clutter)
    return η

def gen_dataset_sup_split(ξ, M, r, n_split, L):
    P = np.shape(ξ)[0]; N = np.shape(ξ)[1]
    num_per_example = int(M//n_split)
    probs = np.random.uniform(size=(L, n_split, num_per_example, N, P))
    χ = np.sign(0.5*(1+r)-probs)
    η = np.array(tf.einsum('akmiK,Ki->akmKi', χ, ξ))
    η = tf.convert_to_tensor(η, dtype=np.float32)
    return η


def unsupervised_J(η, M):
    N = np.shape(η)[1]
    return tf.einsum('ai,aj->ij', η, η)/(N*M)

def supervised_J(η):
    η = tf.convert_to_tensor(η, dtype=np.float32)
    M, K, N = np.shape(η)
    ξ = np.sign(np.sum(η, axis=0))
    return tf.einsum('ki,kj->ij', ξ, ξ)/N


def JK_real_sup(ξ):
    N = ξ.shape[1]
    C = (tf.einsum('ki,pi->kp', ξ, ξ)/N + tf.einsum('ki,pi->pk', ξ, ξ)/N)/2
    C_inv = np.linalg.inv(C)
    J = tf.einsum('ki,kp,pj->ij', ξ, C_inv, ξ)/N
    return J


def JK_real_unsup(η, M):
    N = η.shape[1]
    C = tf.einsum('ki,pi->kp', η, η)/(M*N)
    C_inv = np.linalg.inv(C)
    J = tf.einsum('ki,kp,pj->ij', η, C_inv, η)/(N*M)
    return J
########################################################
########################################################


########################################################
# K_eff ESTIMATION UTILITIES (MP / SHUFFLE THRESHOLDS)
########################################################
def estimate_K_eff_from_J(J, method="mp", alpha=0.01, n_random=32, M_eff=None, data_var=None):
    """Estimate effective rank (K_eff) of a symmetric connectivity / covariance-like matrix.

    Parameters
    ----------
    J : (N, N) array
        Symmetric matrix (e.g. propagated JKS).
    method : {'shuffle','mp'}
        'shuffle'  -> build null distribution from random symmetric matrices using sampled off-diagonal entries.
        'mp'       -> use Marchenko-Pastur upper edge (requires M_eff, the effective sample size used to build J or underlying covariance before projection).
    alpha : float
        Significance level for threshold (1-alpha quantile for shuffle; ignored for MP except via edge detection).
    n_random : int
        Number of random matrices for shuffle null model.
    M_eff : int or None
        Effective number of samples (needed for MP). If None and method='mp', a ValueError is raised.
    data_var : float or None
        Optional variance estimate of noise. If None, estimated from lower 50% eigenvalues.

    Returns
    -------
    K_eff : int
        Estimated number of informative components.
    keep_mask : boolean array
        Mask over eigenvalues (descending) marking retained ones.
    info : dict
        Auxiliary diagnostic values (eigenvalues, threshold, method specifics).

    Notes
    -----
    - For 'shuffle' we preserve the empirical distribution of off-diagonal entries but destroy structure; diagonal is kept.
    - For 'mp' we assume a whitened Wishart-like bulk: λ_max^MP = σ^2 (1 + sqrt(q))^2 with q = N / M_eff.
    """
    J = np.asarray(J)
    # Symmetric eigenvalues (faster and more stable for symmetric J)
    evals = np.linalg.eigvalsh(J)
    # Sort descending for convenience
    order = np.argsort(evals)[::-1]
    evals_sorted = evals[order]

    if method == 'shuffle':
        N = J.shape[0]
        # Collect off-diagonal entries
        off = J[np.triu_indices(N, k=1)]
        diag = np.diag(J)
        rand_max = []
        for _ in range(n_random):
            # Sample without replacement if enough entries else with replacement
            if len(off) >= (N*(N-1))//2:
                sampled = np.random.choice(off, size=(N*(N-1))//2, replace=True)
            else:
                sampled = np.random.choice(off, size=(N*(N-1))//2, replace=True)
            R = np.zeros_like(J)
            iu = np.triu_indices(N, k=1)
            R[iu] = sampled
            R = R + R.T
            np.fill_diagonal(R, diag)  # keep marginal scales similar
            revals = np.linalg.eigvalsh(R)
            rand_max.append(np.max(revals))
        threshold = np.quantile(rand_max, 1 - alpha)
        keep_mask = evals_sorted > threshold
        K_eff = int(np.sum(keep_mask))
        info = {
            'threshold': threshold,
            'rand_max': rand_max,
            'eigenvalues': evals_sorted,
            'method': 'shuffle'
        }
        return K_eff, keep_mask, info

    elif method == 'mp':
        if M_eff is None:
            raise ValueError("M_eff (effective sample size) must be provided for MP method.")
        N = J.shape[0]
        q = N / float(M_eff)
        # Estimate noise variance from lower half eigenvalues if not provided
        if data_var is None:
            lower_half = evals_sorted[len(evals_sorted)//2:]
            data_var = np.median(lower_half)
        lambda_plus = data_var * (1 + np.sqrt(q))**2
        keep_mask = evals_sorted > lambda_plus
        K_eff = int(np.sum(keep_mask))
        info = {
            'lambda_plus': lambda_plus,
            'q': q,
            'data_var': data_var,
            'eigenvalues': evals_sorted,
            'method': 'mp'
        }
        return K_eff, keep_mask, info
    else:
        raise ValueError("method must be 'shuffle' or 'mp'")



# ---- hopfield_eval.py ----
# -*- coding: utf-8 -*-
"""
Valutazione post-hoc con rete di Hopfield (single-mode).

Dato J_server (matrice hebbiana finale del server) e i veri archetipi ξ_true,
simuliamo la dinamica di Hopfield partendo da stati iniziali fortemente corrotti
di ciascun archetipo e misuriamo la magnetizzazione finale. Questo consente di
testare l'ipotesi "archetipi più esposti ⇒ retrieval migliore".

API principali
--------------
- corrupt_like_archetype(...)    : genera σ0 con overlap iniziale controllato.
- run_hopfield_test(...)         : esegue la dinamica Hopfield su più repliche.
- eval_retrieval_vs_exposure(..) : aggrega per archetipo e correla con esposizione.

Compatibilità
-------------
Fa uso di `Hopfield_Network` definita in `src.unsup.networks`. Se desideri
iniettare J_server direttamente (senza passare da prepare(η)), è sufficiente:
    net = Hopfield_Network()
    net.N = J_server.shape[0]
    net.J = J_server
e poi chiamare `net.dynamics(...)`.
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Tuple, Optional, Any, List
from pathlib import Path
import json, time
import os

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.axes import Axes

# Import locale
from src.unsup.networks import Hopfield_Network


__all__ = [
    "corrupt_like_archetype",
    "run_hopfield_test",
    "eval_retrieval_vs_exposure",
    # nuove API di persistenza / plotting
    "run_or_load_hopfield_eval",
    "save_hopfield_eval",
    "load_hopfield_eval",
    "plot_magnetization_distribution",
    "plot_mean_vs_exposure",
]


def corrupt_like_archetype(
    xi_true: np.ndarray,
    reps_per_archetype: int,
    start_overlap: float = 0.3,
    rng: Optional[np.random.Generator] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Genera stati iniziali σ0 per Hopfield come versioni corrotte degli archetipi.

    Parametri
    ---------
    xi_true : (K, N) in {±1}
    reps_per_archetype : int
        Numero di repliche per ciascun archetipo.
    start_overlap : float in [0, 1]
        Overlap atteso iniziale con l'archetipo (0 = rumore puro, 1 = identico).
        Implementazione tramite flip indipendenti con P(flip) = (1 - start_overlap)/2.
    rng : np.random.Generator opzionale

    Returns
    -------
    σ0 : (K * reps_per_archetype, N) in {±1}
        Stati iniziali per la dinamica di Hopfield.
    targets : (K * reps_per_archetype,)
        Indici degli archetipi corrispondenti a ciascuno stato iniziale.
    """
    rng = np.random.default_rng() if rng is None else rng
    K, N = xi_true.shape
    p_flip = (1.0 - float(start_overlap)) * 0.5
    total = K * int(reps_per_archetype)
    σ0 = np.empty((total, N), dtype=int)
    targets = np.empty((total,), dtype=int)

    t = 0
    for μ in range(K):
        flips = rng.random(size=(reps_per_archetype, N)) < p_flip
        # flip bit ⇒ moltiplicare per -1
        σ0[t:t + reps_per_archetype] = np.where(flips, -xi_true[μ], xi_true[μ]).astype(int)
        targets[t:t + reps_per_archetype] = μ
        t += reps_per_archetype
    return σ0, targets


@dataclass
class HopfieldEvalResult:
    """Risultati aggregati della valutazione Hopfield."""
    magnetization_by_mu: Dict[int, np.ndarray]  # μ -> array (reps,)
    mean_by_mu: Dict[int, float]
    std_by_mu: Dict[int, float]
    overall_mean: float
    overall_std: float


def run_hopfield_test(
    J_server: np.ndarray,
    xi_true: np.ndarray,
    beta: float = 3.0,
    updates: int = 30,
    reps_per_archetype: int = 32,
    start_overlap: float = 0.3,
    rng: Optional[np.random.Generator] = None,
    stochastic: bool = True,
) -> HopfieldEvalResult:
    """
    Esegue la dinamica Hopfield su J_server con iniziali corrotti degli archetipi.

    Parametri
    ---------
    J_server : (N, N)
        Matrice sinaptica da valutare (quella finale del server).
    xi_true : (K, N) in {±1}
        Archetipi di riferimento per il calcolo delle magnetizzazioni.
    beta : float
        Inverse temperature per la dinamica (controlla la spinta del segnale).
    updates : int
        Numero di aggiornamenti paralleli.
    reps_per_archetype : int
        Repliche per ogni archetipo.
    start_overlap : float
        Overlap iniziale desiderato con l'archetipo (0..1).
    rng : np.random.Generator opzionale

    Returns
    -------
    HopfieldEvalResult
        Magnetizzazioni per archetipo e statistiche aggregate.
    """
    rng = np.random.default_rng() if rng is None else rng
    K, N = xi_true.shape
    # Stati iniziali e mapping verso il "bersaglio" μ
    σ0, targets = corrupt_like_archetype(xi_true, reps_per_archetype, start_overlap, rng=rng)

    # Prepara rete di Hopfield e inietta direttamente J_server
    net = Hopfield_Network()
    net.N = int(J_server.shape[0])
    net.J = np.asarray(J_server, dtype=np.float32)

    # Dinamica parallela
    net.dynamics(σ0.astype(np.float32), β=beta, updates=updates, mode="parallel", stochastic=stochastic)
    σf = np.asarray(net.σ, dtype=int)
    if σf is None:
        raise RuntimeError("Hopfield_Network.dynamics non ha prodotto stati finali.")

    # Magnetizzazione finale verso il rispettivo target
    # m = |<σf, ξ_target>|/N
    mag_by_mu: Dict[int, list] = {μ: [] for μ in range(K)}
    for i in range(σf.shape[0]):
        μ = int(targets[i])
        m = float(np.abs(np.dot(σf[i], xi_true[μ])) / N)
        mag_by_mu[μ].append(m)

    # Aggrega
    mag_by_mu_np: Dict[int, np.ndarray] = {μ: np.asarray(mag_by_mu[μ], dtype=float) for μ in range(K)}
    mean_by_mu = {μ: float(v.mean()) if v.size else 0.0 for μ, v in mag_by_mu_np.items()}
    std_by_mu = {μ: float(v.std(ddof=1)) if v.size > 1 else 0.0 for μ, v in mag_by_mu_np.items()}
    all_vals = np.concatenate([v for v in mag_by_mu_np.values() if v.size], axis=0) if any(
        v.size for v in mag_by_mu_np.values()
    ) else np.array([0.0])

    return HopfieldEvalResult(
        magnetization_by_mu=mag_by_mu_np,
        mean_by_mu=mean_by_mu,
        std_by_mu=std_by_mu,
        overall_mean=float(all_vals.mean()),
        overall_std=float(all_vals.std(ddof=1)) if all_vals.size > 1 else 0.0,
    )


def _pearson(x: np.ndarray, y: np.ndarray) -> float:
    x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)
    if x.size != y.size or x.size == 0:
        return float("nan")
    xc = x - x.mean(); yc = y - y.mean()
    num = float(np.dot(xc, yc))
    den = float(np.linalg.norm(xc) * np.linalg.norm(yc)) + 1e-12
    return num / den


def _spearman(x: np.ndarray, y: np.ndarray) -> float:
    # rank correlation simple implementation
    def _ranks(a: np.ndarray) -> np.ndarray:
        order = a.argsort()
        ranks = np.empty_like(order, dtype=float)
        ranks[order] = np.arange(1, a.size + 1, dtype=float)
        return ranks
    xr = _ranks(np.asarray(x, dtype=float))
    yr = _ranks(np.asarray(y, dtype=float))
    return _pearson(xr, yr)


def eval_retrieval_vs_exposure(
    J_server: np.ndarray,
    xi_true: np.ndarray,
    exposure_counts: np.ndarray,
    beta: float = 3.0,
    updates: int = 30,
    reps_per_archetype: int = 32,
    start_overlap: float = 0.3,
    rng: Optional[np.random.Generator] = None,
    stochastic: bool = True,
) -> Dict[str, object]:
    """
    Esegue `run_hopfield_test` e correla la magnetizzazione media per archetipo
    con il numero di esposizioni (quante volte l'archetipo è apparso nei round).

    Returns
    -------
    dict con chiavi:
      - 'mean_by_mu'       : dict μ -> float
      - 'std_by_mu'        : dict μ -> float
      - 'pearson'          : float
      - 'spearman'         : float
      - 'overall_mean/std' : float, float
      - 'magnetization_by_mu' : μ -> np.ndarray (tutte le repliche)
    """
    res = run_hopfield_test(
        J_server=J_server,
        xi_true=xi_true,
        beta=beta,
        updates=updates,
        reps_per_archetype=reps_per_archetype,
        start_overlap=start_overlap,
        rng=rng,
        stochastic=stochastic,
    )
    K = xi_true.shape[0]
    expo = np.asarray(exposure_counts, dtype=float).reshape(K)
    means = np.array([res.mean_by_mu.get(μ, 0.0) for μ in range(K)], dtype=float)

    return {
        "mean_by_mu": res.mean_by_mu,
        "std_by_mu": res.std_by_mu,
        "overall_mean": res.overall_mean,
        "overall_std": res.overall_std,
        "pearson": _pearson(expo, means),
        "spearman": _spearman(expo, means),
        "magnetization_by_mu": res.magnetization_by_mu,
    }


# =========================
# Persistenza & Caching
# =========================

def _ensure_dir(p: Path) -> Path:
    p.mkdir(parents=True, exist_ok=True)
    return p


def save_hopfield_eval(
    output_dir: str | os.PathLike,
    eval_dict: Dict[str, Any],
    J_server: np.ndarray,
    xi_true: np.ndarray,
    sigma0: Optional[np.ndarray],
    sigmaf: Optional[np.ndarray],
    params: Dict[str, Any],
    exposure_counts: Optional[np.ndarray] = None,
    overwrite: bool = False,
) -> str:
    """Salva su disco tutti gli artefatti (metadati + matrici).

    Struttura cartella:
      meta.json
      J_server.npy
      xi_true.npy
      (exposure_counts.npy) opzionale
      sigma0.npy, sigmaf.npy (se presenti)
      magnetization_by_mu.npz  (chiavi m_0, m_1, ...)
      stats.json  (mean/std per μ, correlazioni, overall)
    """
    out = Path(output_dir)
    if out.exists() and not overwrite:
        # non sovrascrivere: lasciamo intatto
        return str(out)
    _ensure_dir(out)

    # Meta + parametri
    meta = {
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "params": params,
        "shapes": {
            "J_server": list(J_server.shape),
            "xi_true": list(xi_true.shape),
            "sigma0": list(sigma0.shape) if sigma0 is not None else None,
            "sigmaf": list(sigmaf.shape) if sigmaf is not None else None,
        },
    }
    (out / "meta.json").write_text(json.dumps(meta, indent=2))
    np.save(out / "J_server.npy", np.asarray(J_server, dtype=np.float32))
    np.save(out / "xi_true.npy", xi_true.astype(np.int8))
    if exposure_counts is not None:
        np.save(out / "exposure_counts.npy", np.asarray(exposure_counts, dtype=np.int32))
    if sigma0 is not None:
        np.save(out / "sigma0.npy", sigma0.astype(np.int8))
    if sigmaf is not None:
        np.save(out / "sigmaf.npy", sigmaf.astype(np.int8))

    # Magnetizzazioni per μ
    mag_dict: Dict[int, np.ndarray] = eval_dict.get("magnetization_by_mu", {})  # type: ignore
    if mag_dict:
        _mag_payload = {f"m_{k}": np.asarray(v, dtype=np.float32) for k, v in mag_dict.items()}
        np.savez_compressed(str(out / "magnetization_by_mu.npz"), **_mag_payload)  # type: ignore[arg-type]

    stats_payload = {
        "mean_by_mu": eval_dict.get("mean_by_mu"),
        "std_by_mu": eval_dict.get("std_by_mu"),
        "overall_mean": eval_dict.get("overall_mean"),
        "overall_std": eval_dict.get("overall_std"),
        "pearson": eval_dict.get("pearson"),
        "spearman": eval_dict.get("spearman"),
        "exposure_counts": np.asarray(exposure_counts).tolist() if exposure_counts is not None else None,
    }
    (out / "stats.json").write_text(json.dumps(stats_payload, indent=2))
    return str(out)


def load_hopfield_eval(output_dir: str | os.PathLike) -> Dict[str, Any]:
    """Carica gli artefatti salvati in precedenza.

    Restituisce un dizionario con chiavi:
      J_server, xi_true, exposure_counts (se esiste), sigma0, sigmaf,
      eval (stats + magnetization_by_mu)
      meta
    """
    out = Path(output_dir)
    if not out.exists():
        raise FileNotFoundError(f"Directory non trovata: {out}")
    meta = json.loads((out / "meta.json").read_text()) if (out / "meta.json").exists() else {}
    def _maybe(name: str):
        p = out / name
        return np.load(p) if p.exists() else None
    J_server = _maybe("J_server.npy")
    xi_true = _maybe("xi_true.npy")
    exposure = _maybe("exposure_counts.npy")
    sigma0 = _maybe("sigma0.npy")
    sigmaf = _maybe("sigmaf.npy")
    mag_file = out / "magnetization_by_mu.npz"
    magnetization_by_mu = {}
    if mag_file.exists():
        loaded = np.load(mag_file)
        for k in loaded.files:
            if k.startswith("m_"):
                μ = int(k.split("_", 1)[1])
                magnetization_by_mu[μ] = loaded[k]
    stats = json.loads((out / "stats.json").read_text()) if (out / "stats.json").exists() else {}
    stats["magnetization_by_mu"] = magnetization_by_mu
    return {
        "meta": meta,
        "J_server": J_server,
        "xi_true": xi_true,
        "exposure_counts": exposure,
        "sigma0": sigma0,
        "sigmaf": sigmaf,
        "eval": stats,
    }


def run_or_load_hopfield_eval(
    output_dir: str,
    J_server: Optional[np.ndarray] = None,
    xi_true: Optional[np.ndarray] = None,
    exposure_counts: Optional[np.ndarray] = None,
    *,
    beta: float = 3.0,
    updates: int = 30,
    reps_per_archetype: int = 32,
    start_overlap: float = 0.3,
    force_run: bool = False,
    save: bool = True,
    rng: Optional[np.random.Generator] = None,
    stochastic: bool = True,
) -> Dict[str, Any]:
    """Esegue (o ricarica) la valutazione Hopfield + correlazioni esposizione.

    Se la cartella esiste e force_run=False la ricarica; altrimenti richiede
    J_server & xi_true per eseguire. Restituisce dizionario come `load_hopfield_eval`.
    """
    out = Path(output_dir)
    if out.exists() and not force_run:
        return load_hopfield_eval(out)
    if J_server is None or xi_true is None:
        raise ValueError("Servono J_server e xi_true per eseguire (force_run=True o cartella assente).")

    # Run principale
    rng = np.random.default_rng() if rng is None else rng
    K, N = xi_true.shape
    sigma0, targets = corrupt_like_archetype(xi_true, reps_per_archetype, start_overlap, rng=rng)
    net = Hopfield_Network()
    net.N = int(J_server.shape[0])
    net.J = np.asarray(J_server, dtype=np.float32)
    net.dynamics(sigma0.astype(np.float32), β=beta, updates=updates, mode="parallel", stochastic=stochastic)
    sigmaf = np.asarray(net.σ, dtype=int)
    if sigmaf is None:
        raise RuntimeError("Dinamica Hopfield fallita: sigma finale None")

    # Magnetizzazione
    mag_by_mu: Dict[int, List[float]] = {μ: [] for μ in range(K)}
    for i in range(sigmaf.shape[0]):
        μ = int(targets[i])
        m = float(np.abs(np.dot(sigmaf[i], xi_true[μ])) / N)
        mag_by_mu[μ].append(m)
    mag_np = {μ: np.asarray(v, dtype=float) for μ, v in mag_by_mu.items()}
    mean_by_mu = {μ: float(v.mean()) if v.size else 0.0 for μ, v in mag_np.items()}
    std_by_mu = {μ: float(v.std(ddof=1)) if v.size > 1 else 0.0 for μ, v in mag_np.items()}
    all_vals = np.concatenate([v for v in mag_np.values() if v.size], axis=0)

    exposure_counts_local = exposure_counts if exposure_counts is not None else np.ones(K)
    expo = np.asarray(exposure_counts_local, dtype=float).reshape(K)
    means = np.array([mean_by_mu.get(μ, 0.0) for μ in range(K)], dtype=float)
    eval_dict = {
        "mean_by_mu": mean_by_mu,
        "std_by_mu": std_by_mu,
        "overall_mean": float(all_vals.mean()),
        "overall_std": float(all_vals.std(ddof=1)) if all_vals.size > 1 else 0.0,
        "pearson": _pearson(expo, means),
        "spearman": _spearman(expo, means),
        "magnetization_by_mu": mag_np,
    }
    params = dict(
        beta=beta,
        updates=updates,
        reps_per_archetype=reps_per_archetype,
        start_overlap=start_overlap,
        stochastic=stochastic,
        force_run=force_run,
    )
    if save:
        save_hopfield_eval(out, eval_dict, J_server, xi_true, sigma0, sigmaf, params, exposure_counts)
    return load_hopfield_eval(out)


# =========================
# Plotting utilities (Seaborn)
# =========================

def _to_series(mag_by_mu: Dict[int, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
    xs, ys = [], []
    for μ, arr in sorted(mag_by_mu.items()):
        xs.extend([μ] * len(arr))
        ys.extend(arr.tolist())
    return np.array(xs), np.array(ys)


def plot_magnetization_distribution(
    eval_artifacts: Dict[str, Any] | Dict[str, np.ndarray],
    ax: Optional[Axes] = None,
    palette: str = "viridis",
    jitter: float = 0.05,
    title: Optional[str] = None,
) -> Axes:
    """Box + swarm dei magnetizzazioni per archetipo.

    Parametri
    ---------
    eval_artifacts : dizionario che include 'magnetization_by_mu' (ad es. output['eval']).
    jitter : ampiezza jitter per scatter (se 0 non viene effettuato).
    """
    mag_by_mu_any = eval_artifacts.get("magnetization_by_mu", eval_artifacts.get("eval", {}).get("magnetization_by_mu"))  # type: ignore
    if mag_by_mu_any is None:
        raise ValueError("Nessun campo 'magnetization_by_mu' trovato.")
    # enforce dict[int, np.ndarray]
    if isinstance(mag_by_mu_any, dict):
        mag_by_mu: Dict[int, np.ndarray] = {int(k): np.asarray(v) for k, v in mag_by_mu_any.items()}
    else:
        raise TypeError("'magnetization_by_mu' deve essere un dict. File salvato corrotto?")
    xs, ys = _to_series(mag_by_mu)
    df = {"archetipo": xs, "mag": ys}
    import pandas as pd
    df = pd.DataFrame(df)
    if ax is None:
        _, ax = plt.subplots(figsize=(6, 4))
    # Use violin plot on the left as requested, keep individual points as jittered stripplot
    # inner='quartile' shows median and quartiles inside the violin; cut=0 avoids extended tails
    sns.violinplot(data=df, x="archetipo", y="mag", ax=ax, palette=palette, inner="quartile", cut=0)
    if jitter > 0:
        sns.stripplot(data=df, x="archetipo", y="mag", ax=ax, color="k", alpha=0.5, size=3, jitter=jitter)
    ax.axhline(1.0, ls="--", c="gray", lw=1)
    ax.set_ylabel("Magnetizzazione finale |m|")
    ax.set_xlabel("Archetipo μ")
    if title:
        ax.set_title(title)
    sns.despine(ax=ax)
    return ax


def plot_mean_vs_exposure(
    eval_artifacts: Dict[str, Any],
    exposure_counts: Optional[np.ndarray] = None,
    ax: Optional[Axes] = None,
    palette: str = "mako",
    annotate: bool = True,
    title: Optional[str] = None,
) -> Axes:
    """Scatter (mean magnetization vs exposure) con regressione lineare."""
    means_dict = eval_artifacts.get("mean_by_mu", eval_artifacts.get("eval", {}).get("mean_by_mu"))  # type: ignore
    if means_dict is None:
        raise ValueError("Mancano 'mean_by_mu'.")
    means = np.array([means_dict[k] for k in sorted(means_dict.keys())], dtype=float)
    K = means.size
    if exposure_counts is None:
        # prova a recuperare
        exposure_counts = eval_artifacts.get("exposure_counts")
        if exposure_counts is None and "eval" in eval_artifacts:
            exposure_counts = eval_artifacts.get("eval", {}).get("exposure_counts")  # type: ignore
    if exposure_counts is None:
        exposure_counts = np.ones(K)
    expo = np.asarray(exposure_counts, dtype=float).reshape(K)
    corr_p = eval_artifacts.get("pearson", eval_artifacts.get("eval", {}).get("pearson"))
    corr_s = eval_artifacts.get("spearman", eval_artifacts.get("eval", {}).get("spearman"))
    if ax is None:
        _, ax = plt.subplots(figsize=(5.5, 4))
    df = {"Exposure": expo, "MeanMag": means, "μ": list(range(K))}
    import pandas as pd
    df = pd.DataFrame(df)
    sns.regplot(data=df, x="Exposure", y="MeanMag", ax=ax, scatter=False, color="black", line_kws={"lw":1, "ls":"--"})
    sns.scatterplot(data=df, x="Exposure", y="MeanMag", hue="μ", ax=ax, palette=palette, s=60)
    ax.set_ylabel("Magnetizzazione media")
    if title:
        ax.set_title(title)
    if annotate:
        txt = f"Pearson={corr_p:.3f}\nSpearman={corr_s:.3f}" if corr_p is not None else ""
        ax.text(0.02, 0.98, txt, transform=ax.transAxes, va="top", ha="left", fontsize=9,
                bbox=dict(boxstyle="round,pad=0.3", fc="white", alpha=0.8, ec="gray"))
    sns.despine(ax=ax)
    return ax



# ---- metrics.py ----
# -*- coding: utf-8 -*-
"""
Metriche core per exp_01 (single):

- Frobenius relativo tra J_KS e J*.
- Overlap/matching Ungherese per retrieval medio.
- Magnetizzazioni per-candidato.
- Coverage per round.
- Z-score robusto (median/MAD).
- Wrapper semplici per serie K_eff (se vuoi agganciarti a spectrum.py).

Formule e definizioni sono allineate alle sezioni *Metrics* e *Disentangling*
(del report): FRO, matching su costo 1-M, magnetizzazioni come max overlap, ecc.
"""
from __future__ import annotations

from typing import Iterable, Tuple, List
import numpy as np


def frobenius_relative(J_hat: np.ndarray, J_star: np.ndarray, eps: float = 1e-9) -> float:
    """
    ||J_hat - J_star||_F / (||J_star||_F + eps)
    """
    num = np.linalg.norm(J_hat - J_star, ord="fro")
    den = np.linalg.norm(J_star, ord="fro") + float(eps)
    return float(num / den)


def overlap_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:
    """
    M_ab = |<A_a, B_b>| / N, dove A.shape=(Ka, N), B.shape=(Kb, N).
    """
    if A.size == 0 or B.size == 0:
        return np.zeros((A.shape[0], B.shape[0]), dtype=float)
    N = A.shape[1]
    return np.abs(A @ B.T) / float(N)


def magnetisations(xi_r: np.ndarray, xi_true: np.ndarray) -> np.ndarray:
    """
    m_a = max_b M_ab
    """
    M = overlap_matrix(xi_r, xi_true)
    return M.max(axis=1) if M.size else np.array([])


def retrieval_mean_hungarian(xi_est: np.ndarray, xi_true: np.ndarray) -> float:
    """
    Retrieval medio dopo matching ottimo (costo 1 - M) via algoritmo Ungherese.

    Se Ka != Kb, normalizza in modo conservativo come nel codice degli esperimenti:
    - se Ka < Kb: somma overlaps matched / Kb
    - altrimenti: media semplice degli overlaps matched
    """
    from scipy.optimize import linear_sum_assignment

    Ka, N = xi_est.shape
    Kb, N2 = xi_true.shape
    if N != N2:
        raise ValueError("Dimensioni incompatibili per overlap.")
    M = overlap_matrix(xi_est, xi_true)
    cost = 1.0 - M
    rI, cI = linear_sum_assignment(cost)
    overlaps = M[rI, cI]
    if Ka < Kb:
        return float(overlaps.sum() / Kb)
    return float(overlaps.mean())


def coverage_fraction(labels_seen: Iterable[int], K: int) -> float:
    """
    Coverage istantaneo di un round: frazione di archetipi distinti visti (∈ [0,1]).
    """
    S = set(int(x) for x in labels_seen)
    return float(len(S) / max(1, K))


def robust_z(values: Iterable[float]) -> List[float]:
    """
    Z-score robusto: (x - med) / (1.4826 * MAD)
    Restituisce una lista per compatibilità con pipelines di aggregazione.
    """
    x = np.asarray(list(values), dtype=float)
    med = np.median(x)
    mad = np.median(np.abs(x - med)) + 1e-12
    z = (x - med) / (1.4826 * mad)
    return list(z)


# --- (Opzionali) util per serie Keff: questi sono thin wrapper se vuoi usare spectrum.py ---

def keff_from_eigs(vals: np.ndarray, mp_edge: float) -> int:
    """
    Conta # { λ_i > mp_edge } (comodo se hai già calcolato edge MP altrove).
    """
    return int(np.count_nonzero(np.asarray(vals, float) > float(mp_edge)))


# ---- mnist_hfl.py ----
# -*- coding: utf-8 -*-
"""
Adapter per dataset reali (MNIST) in modalità HFL (single-mode).

Funzioni principali:
- load_mnist(...)                  : caricamento sicuro (facoltativo, via torchvision se disponibile)
- binarize_images(...)             : binarizzazione in {±1} con threshold
- class_prototypes_sign_mean(...)  : archetipi da media di classe (sign(mean))
- build_class_mapping(...)         : mappa class_label -> archetype index (0..K-1)
- make_mnist_hfl_subsets(...)      : sottoinsiemi di classi per client (come nel tuo esempio)
- gen_dataset_from_mnist_single(...) : genera ETA/labels (L, T, M_c, N) pescando immagini reali

Nota: questo modulo NON usa la generazione sintetica da ξ_true; qui i campioni
ETA sono immagini MNIST binarizzate. Le labels sono gli indici di archetipo
secondo la mappatura class->arch costruita per l'esperimento.
"""
from __future__ import annotations

from typing import Dict, Iterable, List, Sequence, Tuple, Optional

import numpy as np


__all__ = [
    "load_mnist",
    "binarize_images",
    "class_prototypes_sign_mean",
    "build_class_mapping",
    "make_mnist_hfl_subsets",
    "gen_dataset_from_mnist_single",
]


def load_mnist(root: str, train: bool = True) -> Tuple[np.ndarray, np.ndarray]:
    """
    Carica MNIST tramite torchvision se disponibile. Restituisce (X, y):
      X: (num, 28, 28) uint8
      y: (num,) int64
    """
    try:
        from torchvision.datasets import MNIST
        import torchvision.transforms as T
    except Exception as e:
        raise ImportError(
            "torchvision non disponibile. Installa torchvision oppure "
            "fornisci direttamente (X, y) alle altre funzioni."
        ) from e

    ds = MNIST(root=root, train=train, download=True, transform=None)
    # ds.data: (num, 28, 28) uint8; ds.targets: (num,) int64
    X = ds.data.numpy()
    y = ds.targets.numpy()
    return X, y


def binarize_images(X: np.ndarray, threshold: float = 0.5) -> np.ndarray:
    """
    Binarizza immagini grayscale in {±1}.

    Accetta:
      - uint8 0..255 (scala automaticamente a 0..1)
      - float 0..1
      - shape (num, H, W) o (num, H, W, 1)

    Restituisce:
      - (num, N) int ∈ {±1}, flatten row-wise.
    """
    X = np.asarray(X)
    if X.ndim not in (3, 4):
        raise ValueError("X deve avere shape (num,H,W) o (num,H,W,1).")
    if X.ndim == 4:
        X = X[..., 0]
    X = X.astype(np.float32)
    if X.max() > 1.0:
        X = X / 255.0
    H, W = X.shape[1], X.shape[2]
    Xf = X.reshape(X.shape[0], H * W)
    thr = float(threshold)
    binX = np.where(Xf >= thr, 1, -1).astype(int)
    return binX


def class_prototypes_sign_mean(
    X_bin: np.ndarray,  # (num, N) in {±1}
    y: np.ndarray,      # (num,)
    classes: Sequence[int],
) -> np.ndarray:
    """
    Costruisce prototipi di classe come sign(mean) (pseudo-archetipi).
    Ritorna ξ_prot con righe ordinate secondo 'classes' (len K, N).
    """
    X_bin = np.asarray(X_bin, dtype=int)
    y = np.asarray(y, dtype=int)
    N = X_bin.shape[1]
    protos = []
    for c in classes:
        idx = np.where(y == int(c))[0]
        if idx.size == 0:
            raise ValueError(f"Nessuna immagine per classe {c}.")
        m = X_bin[idx].mean(axis=0)
        xi = np.where(m >= 0.0, 1, -1).astype(int)
        protos.append(xi)
    return np.vstack(protos).reshape(len(classes), N)


def build_class_mapping(classes: Sequence[int]) -> Tuple[Dict[int, int], Dict[int, int]]:
    """
    Mappa bidirezionale tra class label MNIST e archetype index (0..K-1).
    Restituisce: (class_to_arch, arch_to_class)
    """
    unique_sorted = [int(c) for c in sorted(set(classes))]
    class_to_arch = {c: i for i, c in enumerate(unique_sorted)}
    arch_to_class = {i: c for c, i in class_to_arch.items()}
    return class_to_arch, arch_to_class


def make_mnist_hfl_subsets(
    L: int,
    client_classes: Sequence[Sequence[int]],
) -> List[List[int]]:
    """
    Sottoinsiemi di CLASSI per ciascun client, già specificati (es. [[1,2,3],[4,5,6],[7,8,9]]).
    Ritorna una lista di liste (lunghezza L), con classi ordinate e univoche per client.

    Nota: QUI si parla di classi originali MNIST, non di indici archetipo.
    """
    if len(client_classes) != L:
        raise ValueError("client_classes deve avere lunghezza L.")
    subsets: List[List[int]] = []
    for l in range(L):
        uniq_sorted = sorted(set(int(c) for c in client_classes[l]))
        if len(uniq_sorted) == 0:
            raise ValueError(f"Client {l} senza classi assegnate.")
        subsets.append(uniq_sorted)
    return subsets


def _sample_indices_by_class(
    y: np.ndarray,
    allowed_classes: Sequence[int],
    num: int,
    rng: np.random.Generator,
) -> np.ndarray:
    """
    Estrae 'num' indici di immagini dalle classi consentite (equiprobabile sulle classi).
    """
    allowed = list(allowed_classes)
    if len(allowed) == 0:
        raise ValueError("allowed_classes vuoto.")
    # seleziona la classe uniformemente, poi un indice casuale da quella classe
    per_class_idx = {c: np.where(y == c)[0] for c in allowed}
    for c, idx in per_class_idx.items():
        if idx.size == 0:
            raise ValueError(f"Nessuna immagine per classe {c}.")
    cls_choices = rng.integers(0, len(allowed), size=num)
    out = np.empty((num,), dtype=int)
    for i, r in enumerate(cls_choices):
        c = allowed[int(r)]
        pool = per_class_idx[c]
        j = int(rng.integers(0, pool.size))
        out[i] = pool[j]
    return out


def gen_dataset_from_mnist_single(
    X: np.ndarray,                  # immagini raw (num, H, W) o (num,H,W,1)
    y: np.ndarray,                  # labels raw (num,)
    client_classes: Sequence[Sequence[int]],  # classi per client (MNIST labels)
    n_batch: int,
    L: int,
    M_total: int,
    class_to_arch: Dict[int, int],  # mappa: classe MNIST -> indice archetipo [0..K-1]
    rng: Optional[np.random.Generator] = None,
    binarize_threshold: float = 0.5,
    use_tqdm: bool = False,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Genera dataset federato SINGLE usando immagini reali MNIST.

    Restituisce:
      ETA   : (L, n_batch, M_c, N)  campioni binarizzati in {±1}
      labels: (L, n_batch, M_c)     indici archetipo (secondo class_to_arch)
    """
    rng = np.random.default_rng() if rng is None else rng
    X_bin = binarize_images(X, threshold=binarize_threshold)  # (num, N)
    num, N = X_bin.shape

    if len(client_classes) != L:
        raise ValueError("client_classes deve avere lunghezza L.")
    if n_batch <= 0:
        raise ValueError("n_batch deve essere > 0.")
    M_c = int(np.ceil(M_total / float(L * n_batch)))

    ETA = np.zeros((L, n_batch, M_c, N), dtype=np.int32)
    labels = np.zeros((L, n_batch, M_c), dtype=np.int32)

    itL = range(L)
    if use_tqdm:
        try:
            from tqdm import tqdm as _tqdm  # type: ignore
            itL = _tqdm(itL, desc="MNIST clients", leave=False)
        except Exception:
            pass

    for l in itL:
        allowed = list(client_classes[l])
        if len(allowed) == 0:
            raise ValueError(f"Client {l} senza classi consentite.")
        itT = range(n_batch)
        if use_tqdm:
            try:
                from tqdm import tqdm as _tqdm  # type: ignore
                itT = _tqdm(itT, desc=f"client {l} rounds", leave=False)
            except Exception:
                pass
        for t in itT:
            idxs = _sample_indices_by_class(y, allowed_classes=allowed, num=M_c, rng=rng)
            ETA[l, t] = X_bin[idxs]
            # mappa le classi MNIST verso indici archetipo
            labs = [class_to_arch[int(y[i])] for i in idxs]
            labels[l, t] = np.asarray(labs, dtype=np.int32)

    return ETA.astype(np.float32), labels.astype(np.int32)


# ---- networks.py ----
import numpy as np
try:
    from tqdm import tqdm as _tqdm  # type: ignore
except Exception:  # fallback no-op
    _tqdm = None  # type: ignore

# Minimal NumPy backend implementing only the used TensorFlow-like ops
class _Backend:
    def einsum(self, subscripts, *operands, **kwargs):
        # Fast paths for common contractions used in this project
        try:
            if subscripts == 'ai,aj->ij' and len(operands) == 2:
                A, B = operands
                return A.T @ B
            if subscripts == 'ij,Aj->Ai' and len(operands) == 2:
                J, S = operands
                return S @ J.T
            if subscripts == 'ki,pi->kp' and len(operands) == 2:
                A, B = operands
                return A @ B.T
            if subscripts == 'ki,pi->pk' and len(operands) == 2:
                A, B = operands
                return B @ A.T
            if subscripts == 'ki,kp,pj->ij' and len(operands) == 3:
                A, C, B = operands
                return (A.T @ C) @ B
            if subscripts == 'ij,slj->sli' and len(operands) == 2:
                J, S = operands
                return np.tensordot(S, J, axes=([2], [1]))
        except Exception:
            pass
        # Default: enable optimized contraction planning
        if 'optimize' not in kwargs:
            kwargs['optimize'] = True
        return np.einsum(subscripts, *operands, **kwargs)
    def sign(self, x):
        x = np.sign(x)
        return np.where(x >= 0, 1, -1)
    def tanh(self, x):
        return np.tanh(x)
    def convert_to_tensor(self, x, dtype=None):
        return np.array(x, dtype=dtype if dtype is not None else None)
    def transpose(self, x, perm=None):
        return np.transpose(x, axes=perm)

tf = _Backend()


class Hopfield_Network:
    def __init__(self):
        self.N = None
        self.K = None
        self.J = None
        self.σ = None

    def prepare(self, η):
        self.N = η.shape[1]
        self.K = η.shape[0]
        self.J = tf.einsum('ai,aj->ij', η, η) / self.N

    def dynamics(self, σ0, β, updates, mode="parallel", stochastic: bool = True, rng=None):
        """Classic Hopfield dynamics.

        Two variants:
          - deterministic: synchronous updates σ <- sign(J σ)
          - stochastic (default): probabilistic parallel Glauber step using tanh(β h).

        Parameters
        ----------
        σ0 : (M, N) initial states
        β : inverse temperature (effective only if stochastic=True)
        updates : number of update sweeps
        mode : 'parallel' | 'serial'
        stochastic : if True use probabilistic updates with probabilities (1 + tanh(β h))/2
        rng : optional np.random.Generator
        """
        assert self.N is not None, "Call prepare first"
        rng = np.random.default_rng() if rng is None else rng
        N = self.N
        M = σ0.shape[0]
        J = self.J
        σ = tf.convert_to_tensor(σ0, dtype=np.float32)
        for _ in range(updates):
            h = tf.einsum('ij,Aj->Ai', J, σ)
            if mode == "parallel":
                if stochastic:
                    # Probabilities via Glauber dynamics
                    p = (1.0 + np.tanh(β * h)) * 0.5
                    σ = (rng.random(size=(M, N)) < p).astype(np.float32)
                    σ = 2 * σ - 1  # map {0,1} -> {-1, +1}
                else:
                    σ = tf.sign(h)
            else:  # serial Glauber
                idx = rng.integers(0, N)
                if stochastic:
                    p = (1.0 + np.tanh(β * h[:, idx])) * 0.5
                    flip = (rng.random(size=(M,)) < p).astype(np.float32)
                    σ[:, idx] = 2 * flip - 1
                else:
                    σ[:, idx] = tf.sign(h[:, idx])
            self.σ = σ


class TAM_Network:
    def __init__(self):
        self.N = None
        self.J = None
        self.L = None
        self.σ = None

    def prepare(self, J, L):
        self.N = J.shape[1]
        self.J = J
        self.L = L

    def compute_fields(self, input_field):
        # σ shape: (s, L, N)
        J = self.J
        σ = self.σ
        N = self.N
        # h1: local fields for each layer
        h1 = tf.einsum('ij,slj->sli', J, σ)
        # Replicate original (possibly unconventional) tensor algebra
        temp0 = tf.einsum('sli,ski->slk', σ, h1)
        temp1 = tf.einsum('slk,sli->ski', temp0, h1)
        temp2 = tf.einsum('skk,ski->ski', temp0, h1)
        h2 = (temp1 - temp2) / N
        h3 = input_field
        return h1, h2, h3

    def dynamics(self, σr, β, λ, h, updates, noise_scale: float = 0.3, anneal: bool = True, min_scale: float = 0.02, schedule: str = "linear", show_progress: bool = False, desc: str = "TAM dyn"):
        """Run TAM dynamics with controlled / annealed noise.

        Parameters
        ----------
        σr : array (s, L, N)
            Initial candidate states.
        β : float
            Inverse temperature parameter (gain for tanh fields).
        λ : float
            Coupling weight for higher-order correction (h2 term).
        h : float
            External input coupling (h3 term).
        updates : int
            Number of parallel update steps.
        noise_scale : float
            Initial scale (std-like) for uniform noise in [-scale, scale]. Much smaller than legacy 1.0.
        anneal : bool
            If True reduce noise over iterations.
        min_scale : float
            Lower bound for noise scale if annealing.
        schedule : {'linear','exp'}
            Annealing schedule shape.
        """
        assert self.N is not None and self.L is not None, "Call prepare first"
        σr = tf.convert_to_tensor(σr, dtype=np.float32)  # expected shape (s, L, N)
        self.σ = np.copy(σr)
        s, L, N = self.σ.shape
        iterator = range(updates)
        if show_progress and _tqdm is not None:
            iterator = _tqdm(iterator, desc=desc, leave=False)
        for t in iterator:
            h1, h2, h3 = self.compute_fields(σr)
            ht = h1 - λ * h2 + h * h3
            if anneal and updates > 1:
                if schedule == "linear":
                    scale = noise_scale - (noise_scale - min_scale) * (t / (updates - 1))
                else:  # exponential
                    # decay so that at final step ~min_scale
                    γ = (min_scale / noise_scale) ** (1 / max(updates - 1, 1))
                    scale = noise_scale * (γ ** t)
            else:
                scale = noise_scale
            # Uniform noise scaled & additionally attenuated by 1/β so large β => effectively lower noise
            eff_scale = scale / max(β, 1e-6)
            noise = np.random.uniform(-eff_scale, eff_scale, size=(s, L, N))
            self.σ = tf.sign(tf.tanh(β * ht) + noise)
            σr = self.σ


# ---- runner_single.py ----
# -*- coding: utf-8 -*-
"""
Runner per exp_01 in modalità SINGLE (multi-round, multi-seed).

Pipeline per seed
-----------------
1) Genera archetipi veri ξ_true (K, N).
2) Costruisci subset per client (coverage parziale, non disgiunto).
3) Genera ETA e labels in SINGLE (L, T, M_c, N).
4) Per ogni round t:
   - Estima J_unsup su ETA[:, t, :, :]
   - Blend con memoria ebraica J(ξ_prev) (se t>0) con peso w
   - Propagation pseudo-inversa J -> J_KS
   - Cut spettrale (τ) e disentangling (TAM) → ξ_r^(t), magnetizzazioni
   - Metriche: FRO(J_KS,J*), retrieval (Hungarian), K_eff (shuffle/MP), coverage(t)
   - Aggiorna memoria ξ_prev per round t+1
5) Restituisce serie per-round e oggetti finali (J_server, ξ_ref).

Opzionali
---------
- Salvataggi JSON/CSV/PNG se `out_dir` è fornita.
- Hopfield post-hoc (eval_retrieval_vs_exposure) come step separato (non abilitato di default).

Compatibilità
-------------
Dipende dai moduli:
  - src.unsup.config       : HyperParams
  - src.unsup.data         : make_client_subsets, gen_dataset_partial_archetypes, new_round_single, compute_round_coverage, count_exposures
  - src.unsup.estimators   : build_unsup_J_single, blend_with_memory
  - src.unsup.spectrum     : eigen_cut, estimate_keff
  - src.unsup.metrics      : frobenius_relative, retrieval_mean_hungarian
  - src.unsup.functions    : propagate_J, JK_real
"""
from __future__ import annotations

import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import subprocess
import sys

# --- imports locali ---
from src.unsup.config import HyperParams
from src.unsup.data import (
    make_client_subsets,
    gen_dataset_partial_archetypes,
    new_round_single,
    compute_round_coverage,
    count_exposures,
)
from src.unsup.estimators import build_unsup_J_single, blend_with_memory
from src.unsup.spectrum import eigen_cut as spectral_cut, estimate_keff
from src.unsup.metrics import frobenius_relative, retrieval_mean_hungarian
from src.unsup.dynamics import dis_check
from src.unsup.functions import gen_patterns, propagate_J, JK_real


__all__ = ["run_exp01_single"]


@dataclass
class RoundLog:
    retrieval: float
    fro: float
    keff: int
    coverage: float


@dataclass
class SeedResult:
    seed: int
    series: List[RoundLog]
    J_server_final: np.ndarray
    xi_ref_final: np.ndarray
    exposure_counts: np.ndarray
    xi_true: Optional[np.ndarray] = None  # aggiunto per uso post-hoc (Hopfield)


def _ensure_outdir(out_dir: Optional[str]) -> Optional[Path]:
    if out_dir is None:
        return None
    p = Path(out_dir)
    p.mkdir(parents=True, exist_ok=True)
    return p


def _save_jsonl(path: Path, rows: List[Dict[str, object]]) -> None:
    with path.open("w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")


def _save_hyperparams(path: Path, hp: HyperParams) -> None:
    with path.open("w", encoding="utf-8") as f:
        json.dump(hp.__dict__, f, ensure_ascii=False, indent=2, default=lambda o: o.__dict__)


def _plot_series(
    path_png: Path,
    x: np.ndarray,
    r_mean: np.ndarray,
    r_se: np.ndarray,
    f_mean: np.ndarray,
    f_se: np.ndarray,
    k_mean: np.ndarray,
    k_se: np.ndarray,
    c_mean: np.ndarray,
    c_se: np.ndarray,
) -> None:
    try:
        import matplotlib.pyplot as plt
    except Exception:
        return

    # Try to use seaborn if available for nicer defaults
    try:
        import seaborn as sns
        sns.set_theme(style="whitegrid")
        palette = sns.color_palette("tab10")
    except Exception:
        sns = None
        palette = None

    fig, axes = plt.subplots(2, 2, figsize=(11, 7))
    ax = axes[0, 0]
    ax.errorbar(x, r_mean, yerr=r_se, marker='.', capsize=3, color=(palette[0] if palette is not None else None))
    ax.set_title("Retrieval (mean)", fontsize=12)

    ax = axes[0, 1]
    ax.errorbar(x, f_mean, yerr=f_se, marker='.', capsize=3, color=(palette[1] if palette is not None else None))
    ax.set_title("Frobenius (relative)", fontsize=12)

    ax = axes[1, 0]
    ax.errorbar(x, k_mean, yerr=k_se, marker='.', capsize=3, color=(palette[2] if palette is not None else None))
    # LaTeX-like label for K_eff
    ax.set_title(r"$K_{\mathrm{eff}}$", fontsize=13)

    ax = axes[1, 1]
    ax.errorbar(x, c_mean, yerr=c_se, marker='.', capsize=3, color=(palette[3] if palette is not None else None))
    ax.set_title("Coverage", fontsize=12)

    # set descriptive y-labels; show x-labels only on bottom row
    axes[0, 0].set_ylabel("retrieval")
    axes[0, 1].set_ylabel("Frobenius (relative)")
    axes[1, 0].set_ylabel(r"$K_{\mathrm{eff}}$")
    axes[1, 1].set_ylabel("coverage")
    for ax in axes[1, :]:
        ax.set_xlabel("round", fontsize=10)

    # ensure tick labels visible and set reasonable font sizes
    for ax in axes.ravel():
        ax.tick_params(axis='both', which='major', labelsize=9)

    # increase margins so y-labels are not cropped when saving
    fig.subplots_adjust(left=0.12, right=0.98, top=0.95, bottom=0.08, hspace=0.28, wspace=0.28)
    try:
        plt.savefig(path_png, dpi=150)
    finally:
        plt.close(fig)


def _load_existing(out_path: Path, hp: HyperParams, do_plot: bool) -> Optional[Dict[str, object]]:
    """Se presenti file salvati (hyperparams.json + log.jsonl) ricostruisce le metriche.

    Ritorna un dict nello stesso formato finale di `run_exp01_single` (eccetto che le liste
    final_J_list/final_xi_list/exposure_list saranno vuote se non disponibili). Se i file
    non esistono restituisce None.
    """
    hp_json = out_path / "hyperparams.json"
    log_jsonl = out_path / "log.jsonl"
    if not (hp_json.exists() and log_jsonl.exists()):
        return None
    try:
        saved_hp = json.loads(hp_json.read_text(encoding="utf-8"))
        # Verifica coerenza parametri chiave; se differiscono, non riusare.
        key_check = ["L", "K", "N", "n_batch", "n_seeds"]
        for k in key_check:
            if k in saved_hp and getattr(hp, k) != saved_hp[k]:
                # Incompatibile → non riuso (nuova configurazione)
                return None
        per_seed_rows = []
        with log_jsonl.open("r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                row = json.loads(line)
                per_seed_rows.append(row)
        if not per_seed_rows:
            return None
        T = len(per_seed_rows[0]["series"])
        n_seeds = len(per_seed_rows)
        arr_retr = np.zeros((n_seeds, T)); arr_fro = np.zeros((n_seeds, T))
        arr_keff = np.zeros((n_seeds, T)); arr_cov = np.zeros((n_seeds, T))
        per_seed: List[SeedResult] = []
        for si, row in enumerate(per_seed_rows):
            series_logs: List[RoundLog] = []
            for t, s in enumerate(row["series"]):
                arr_retr[si, t] = s["retrieval"]
                arr_fro[si, t] = s["fro"]
                arr_keff[si, t] = s["keff"]
                arr_cov[si, t] = s["coverage"]
                series_logs.append(RoundLog(
                    retrieval=float(s["retrieval"]),
                    fro=float(s["fro"]),
                    keff=int(s["keff"]),
                    coverage=float(s["coverage"]),
                ))
            per_seed.append(SeedResult(
                seed=int(row.get("seed", -1)),
                series=series_logs,
                J_server_final=np.empty((0,0), dtype=np.float32),  # non salvato
                xi_ref_final=np.empty((0,0), dtype=int),            # non salvato
                exposure_counts=np.empty(0, dtype=int),             # non salvato
                xi_true=None,
            ))
        def se(a: np.ndarray) -> np.ndarray:
            return a.std(axis=0, ddof=1) / np.sqrt(n_seeds) if n_seeds > 1 else np.zeros(a.shape[1])
        agg = {
            "retrieval_mean": arr_retr.mean(0).tolist(),
            "fro_mean": arr_fro.mean(0).tolist(),
            "keff_mean": arr_keff.mean(0).tolist(),
            "coverage_mean": arr_cov.mean(0).tolist(),
            "retrieval_se": se(arr_retr).tolist(),
            "fro_se": se(arr_fro).tolist(),
            "keff_se": se(arr_keff).tolist(),
            "coverage_se": se(arr_cov).tolist(),
        }
        if do_plot:
            try:
                x = np.arange(T)
                img_path = out_path / "fig_metrics.png"
                _plot_series(
                    img_path,
                    x,
                    np.asarray(agg["retrieval_mean"]),
                    np.asarray(agg["retrieval_se"]),
                    np.asarray(agg["fro_mean"]),
                    np.asarray(agg["fro_se"]),
                    np.asarray(agg["keff_mean"]),
                    np.asarray(agg["keff_se"]),
                    np.asarray(agg["coverage_mean"]),
                    np.asarray(agg["coverage_se"]),
                )
                # try to open the generated image (Windows: os.startfile, else fallback)
                try:
                    if os.name == "nt":
                        os.startfile(str(img_path))
                    else:
                        opener = "open" if sys.platform == "darwin" else "xdg-open"
                        subprocess.run([opener, str(img_path)], check=False)
                except Exception:
                    pass
            except Exception:
                pass
        return {
            "hp": saved_hp,
            "per_seed": per_seed,
            "aggregate": agg,
            "final_J_list": [],
            "final_xi_list": [],
            "exposure_list": [],
        }
    except Exception:
        return None


def run_exp01_single(
    hp: HyperParams,
    seeds: Optional[List[int]] = None,
    out_dir: Optional[str] = None,
    do_plot: bool = True,
    reuse_saved: bool = True,
    force_run: bool = False,
) -> Dict[str, object]:
    """
    Esegue exp_01 in modalità SINGLE su una lista di seed.

    Returns
    -------
    dict con chiavi:
      - 'hp'           : hyperparams (dict)
      - 'per_seed'     : lista SeedResult (serializzabile in parte)
      - 'aggregate'    : medie per round delle serie (retrieval, fro, keff, coverage)
      - 'final_J_list' : lista di J_server_final per seed
      - 'final_xi_list': lista di xi_ref_final per seed
      - 'exposure_list': lista di exposure_counts per seed
    """
    if hp.mode != "single":
        raise ValueError("Runner bloccato alla modalità 'single'.")

    out_path = _ensure_outdir(out_dir)

    # Tentativo riuso risultati già salvati (se richiesto)
    if (out_path is not None) and reuse_saved and not force_run:
        reused = _load_existing(out_path, hp, do_plot=do_plot)
        if reused is not None:
            return reused

    # lista seeds
    if seeds is None:
        seeds = [hp.seed_base + i for i in range(hp.n_seeds)]

    per_seed: List[SeedResult] = []

    # Optional multithread path over seeds; controlled by hp.n_workers (default 1)
    try:
        _n_workers = int(getattr(hp, 'n_workers', 1))
    except Exception:
        _n_workers = 1
    if _n_workers < 1:
        _n_workers = 1
    if _n_workers > 1 and len(seeds) > 1:
        from concurrent.futures import ThreadPoolExecutor

        def _run_one_seed(seed: int) -> SeedResult:
            rng = np.random.default_rng(seed)
            # 1) archetipi veri (K,N) 9 NB: functions.gen_patterns(N, P)
            xi_true = np.asarray(gen_patterns(hp.N, hp.K), dtype=np.int32)
            # 2) ideal J* (pseudo-inversa)
            J_star = np.asarray(JK_real(xi_true), dtype=np.float32)

            # 3) subset per client
            if hp.K_per_client is None:
                raise ValueError("K_per_client must be specified for partial coverage datasets.")
            subsets = make_client_subsets(K=hp.K, L=hp.L, K_per_client=hp.K_per_client, rng=rng)

            # 4) dataset SINGLE
            ETA, labels = gen_dataset_partial_archetypes(
                xi_true=xi_true,
                M_total=hp.M_total,
                r_ex=hp.r_ex,
                n_batch=hp.n_batch,
                L=hp.L,
                client_subsets=subsets,
                rng=rng,
                use_tqdm=hp.use_tqdm,
            )

            # 5) per-round
            series: List[RoundLog] = []
            xi_ref: Optional[np.ndarray] = None
            J_server_final: Optional[np.ndarray] = None

            for t in range(hp.n_batch):
                # Dati round t
                ETA_t = new_round_single(ETA, t)            # (L, M_c, N)
                labels_t = labels[:, t, :]                  # (L, M_c)

                # Stima J unsup & blending memoria
                J_unsup, M_eff = build_unsup_J_single(ETA_t, K=hp.K)  # (N,N), int
                J_rec = blend_with_memory(J_unsup, xi_prev=xi_ref, w=hp.w)

                # Propagation pseudo-inversa (iterazioni da hp.prop.iters)
                J_KS = np.asarray(propagate_J(J_rec, J_real=-1, verbose=False, iters=hp.prop.iters), dtype=np.float32)

                # Cut spettrale & Keff (coerente con SINGLE 9 MP usa M_eff del round)
                V, _k_from_cut, *_ = spectral_cut(J_KS, tau=hp.spec.tau)
                if hp.estimate_keff_method == "mp":
                    K_eff, _, _ = estimate_keff(J_KS, method="mp", M_eff=M_eff)
                else:
                    K_eff, _, _ = estimate_keff(J_KS, method="shuffle")

                # Disentangling (TAM) + magnetizzazioni
                xi_r, m_vec = dis_check(
                    V=V,
                    K=hp.K,
                    L=hp.L,
                    J_rec=J_rec,
                    JKS_iter=J_KS,
                    xi_true=xi_true,
                    tam=hp.tam,
                    spec=hp.spec,
                    show_progress=hp.use_tqdm,
                )

                # Retrieval medio (matching ungherese) e coverage
                retr = retrieval_mean_hungarian(xi_r.astype(int), xi_true.astype(int))
                cov = compute_round_coverage(labels_t, K=hp.K)

                # FRO rispetto a J*
                fro = frobenius_relative(J_KS, J_star)

                series.append(RoundLog(retrieval=retr, fro=fro, keff=int(K_eff), coverage=float(cov)))

                # memoria per round successivo (prendi primi K candidati)
                if xi_r.shape[0] >= hp.K:
                    xi_ref = xi_r[: hp.K].astype(int)
                else:
                    xi_ref = xi_r.astype(int)

                # mantieni ultima J_KS
                J_server_final = J_KS

            assert J_server_final is not None and xi_ref is not None
            expo_counts = count_exposures(labels, K=hp.K)

            return SeedResult(
                seed=seed,
                series=series,
                J_server_final=J_server_final.astype(np.float32),
                xi_ref_final=xi_ref.astype(int),
                exposure_counts=expo_counts.astype(int),
                xi_true=xi_true.astype(int),
            )

        with ThreadPoolExecutor(max_workers=_n_workers) as _ex:
            per_seed = list(_ex.map(_run_one_seed, seeds))
        # Prevent sequential loop below
        seeds = []

    # loop seeds
    for seed in seeds:
        rng = np.random.default_rng(seed)
        # 1) archetipi veri (K,N) — NB: functions.gen_patterns(N, P)
        xi_true = np.asarray(gen_patterns(hp.N, hp.K), dtype=np.int32)
        # 2) ideal J* (pseudo-inversa)
        J_star = np.asarray(JK_real(xi_true), dtype=np.float32)

        # 3) subset per client
        if hp.K_per_client is None:
            raise ValueError("K_per_client must be specified for partial coverage datasets.")
        subsets = make_client_subsets(K=hp.K, L=hp.L, K_per_client=hp.K_per_client, rng=rng)

        # 4) dataset SINGLE
        ETA, labels = gen_dataset_partial_archetypes(
            xi_true=xi_true,
            M_total=hp.M_total,
            r_ex=hp.r_ex,
            n_batch=hp.n_batch,
            L=hp.L,
            client_subsets=subsets,
            rng=rng,
            use_tqdm=hp.use_tqdm,
        )

        # 5) per-round
        series: List[RoundLog] = []
        xi_ref: Optional[np.ndarray] = None
        J_server_final: Optional[np.ndarray] = None

        for t in range(hp.n_batch):
            # Dati round t
            ETA_t = new_round_single(ETA, t)            # (L, M_c, N)
            labels_t = labels[:, t, :]                  # (L, M_c)

            # Stima J unsup & blending memoria
            J_unsup, M_eff = build_unsup_J_single(ETA_t, K=hp.K)  # (N,N), int
            J_rec = blend_with_memory(J_unsup, xi_prev=xi_ref, w=hp.w)

            # Propagation pseudo-inversa (iterazioni da hp.prop.iters)
            J_KS = np.asarray(propagate_J(J_rec, J_real=-1, verbose=False, iters=hp.prop.iters), dtype=np.float32)

            # Cut spettrale & Keff (coerente con SINGLE ⇒ MP usa M_eff del round)
            V, _k_from_cut, *_ = spectral_cut(J_KS, tau=hp.spec.tau)
            if hp.estimate_keff_method == "mp":
                K_eff, _, _ = estimate_keff(J_KS, method="mp", M_eff=M_eff)
            else:
                K_eff, _, _ = estimate_keff(J_KS, method="shuffle")

            # Disentangling (TAM) + magnetizzazioni
            xi_r, m_vec = dis_check(
                V=V,
                K=hp.K,
                L=hp.L,
                J_rec=J_rec,
                JKS_iter=J_KS,
                xi_true=xi_true,
                tam=hp.tam,
                spec=hp.spec,
                show_progress=hp.use_tqdm,
            )

            # Retrieval medio (matching ungherese) e coverage
            retr = retrieval_mean_hungarian(xi_r.astype(int), xi_true.astype(int))
            cov = compute_round_coverage(labels_t, K=hp.K)

            # FRO rispetto a J*
            fro = frobenius_relative(J_KS, J_star)

            series.append(RoundLog(retrieval=retr, fro=fro, keff=int(K_eff), coverage=float(cov)))

            # memoria per round successivo (prendi primi K candidati)
            if xi_r.shape[0] >= hp.K:
                xi_ref = xi_r[: hp.K].astype(int)
            else:
                xi_ref = xi_r.astype(int)

            # mantieni ultima J_KS
            J_server_final = J_KS

        assert J_server_final is not None and xi_ref is not None
        expo_counts = count_exposures(labels, K=hp.K)

        per_seed.append(
            SeedResult(
                seed=seed,
                series=series,
                J_server_final=J_server_final.astype(np.float32),
                xi_ref_final=xi_ref.astype(int),
                exposure_counts=expo_counts.astype(int),
                xi_true=xi_true.astype(int),
            )
        )

    # --- aggregazione sui seed ---
    T = hp.n_batch
    arr_retr = np.zeros((len(per_seed), T), dtype=float)
    arr_fro = np.zeros((len(per_seed), T), dtype=float)
    arr_keff = np.zeros((len(per_seed), T), dtype=float)
    arr_cov = np.zeros((len(per_seed), T), dtype=float)
    for si, sr in enumerate(per_seed):
        for t, rl in enumerate(sr.series):
            arr_retr[si, t] = rl.retrieval
            arr_fro[si, t] = rl.fro
            arr_keff[si, t] = rl.keff
            arr_cov[si, t] = rl.coverage

    agg = {
        "retrieval_mean": arr_retr.mean(axis=0).tolist(),
        "fro_mean": arr_fro.mean(axis=0).tolist(),
        "keff_mean": arr_keff.mean(axis=0).tolist(),
        "coverage_mean": arr_cov.mean(axis=0).tolist(),
        "retrieval_se": (arr_retr.std(axis=0, ddof=1) / np.sqrt(len(per_seed))).tolist() if len(per_seed) > 1 else [0.0] * T,
        "fro_se": (arr_fro.std(axis=0, ddof=1) / np.sqrt(len(per_seed))).tolist() if len(per_seed) > 1 else [0.0] * T,
        "keff_se": (arr_keff.std(axis=0, ddof=1) / np.sqrt(len(per_seed))).tolist() if len(per_seed) > 1 else [0.0] * T,
        "coverage_se": (arr_cov.std(axis=0, ddof=1) / np.sqrt(len(per_seed))).tolist() if len(per_seed) > 1 else [0.0] * T,
    }

    # --- salvataggi opzionali ---
    if out_path is not None:
        # hyperparams
        _save_hyperparams(out_path / "hyperparams.json", hp)

        # log per seed (jsonl: una riga/seed)
        rows = []
        for sr in per_seed:
            rows.append({
                "seed": sr.seed,
                "series": [rl.__dict__ for rl in sr.series],
            })
        _save_jsonl(out_path / "log.jsonl", rows)

        # riassunto csv semplice
        try:
            import csv
            with (out_path / "results_table.csv").open("w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(["round", "retrieval_mean", "fro_mean", "keff_mean", "coverage_mean"])
                for t in range(T):
                    writer.writerow([
                        t,
                        agg["retrieval_mean"][t],
                        agg["fro_mean"][t],
                        agg["keff_mean"][t],
                        agg["coverage_mean"][t],
                    ])
        except Exception:
            pass

        # figura metriche
        if do_plot:
            x = np.arange(T)
            # pass mean and standard-error arrays so the plot shows error bars across seeds
            _plot_series(
                out_path / "fig_metrics.png",
                x,
                np.asarray(agg["retrieval_mean"]),
                np.asarray(agg["retrieval_se"]),
                np.asarray(agg["fro_mean"]),
                np.asarray(agg["fro_se"]),
                np.asarray(agg["keff_mean"]),
                np.asarray(agg["keff_se"]),
                np.asarray(agg["coverage_mean"]),
                np.asarray(agg["coverage_se"]),
            )

        # salva matrici finali J per seed e aggregate (npy/npz), oltre a xi_ref ed exposure se disponibili
        try:
            # salva ogni J finale come J_server_seed_<seed>.npy
            for sr in per_seed:
                try:
                    seed = int(sr.seed)
                    jpath = out_path / f"J_server_seed_{seed}.npy"
                    np.save(str(jpath), sr.J_server_final)
                except Exception:
                    # non bloccare il salvataggio globale per un errore su un seed
                    continue

            # salva array aggregato (n_seeds, N, N) in formato compresso
            try:
                allJ = np.stack([sr.J_server_final for sr in per_seed], axis=0)
                np.savez_compressed(str(out_path / "final_J_list.npz"), final_J=allJ)
            except Exception:
                pass

            # salva xi_ref_final per seed e aggregate
            for sr in per_seed:
                try:
                    np.save(str(out_path / f"xi_ref_seed_{int(sr.seed)}.npy"), sr.xi_ref_final)
                except Exception:
                    continue
            try:
                all_xi = [sr.xi_ref_final for sr in per_seed]
                np.savez_compressed(str(out_path / "final_xi_list.npz"), *all_xi)
            except Exception:
                pass

            # salva exposure counts (n_seeds, K)
            try:
                exp_arr = np.stack([sr.exposure_counts for sr in per_seed], axis=0)
                np.save(str(out_path / "exposure_list.npy"), exp_arr)
            except Exception:
                pass
        except Exception:
            # sicurezza: non interrompere l'esecuzione principale se il salvataggio fallisce
            pass

    return {
        "hp": hp.__dict__,
        "per_seed": per_seed,
        "aggregate": agg,
    "final_J_list": [sr.J_server_final for sr in per_seed],
    "final_xi_list": [sr.xi_ref_final for sr in per_seed],
    "exposure_list": [sr.exposure_counts for sr in per_seed],
    # nuova chiave per rendere disponibili gli archetipi veri
    "xi_true_list": [sr.xi_true for sr in per_seed],
    }


# ---- single_round.py ----
# -*- coding: utf-8 -*-
"""
Orchestratore di UN round in modalità SINGLE.

Fasi per round t:
  1) build_unsup_J_single(ETA_t) -> J_unsup, M_eff
  2) blend_with_memory(J_unsup, xi_prev, w) -> J_rec
  3) propagate_J(J_rec) -> J_KS
  4) eigen_cut(J_KS, tau) -> V
  5) dis_check(V, K, L, J_rec, J_KS, xi_true, tam, spec) -> xi_r, m
  6) metriche: retrieval (Hungarian), FRO, K_eff (shuffle|mp), coverage(labels_t)
  7) aggiorna memoria xi_ref per t+1

Compatibilità:
- Nessun riferimento a modalità 'extend'.
- Usa le stesse primitive già fornite nei moduli precedenti.
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, Tuple

import numpy as np

from src.unsup.config import HyperParams
from src.unsup.estimators import build_unsup_J_single, blend_with_memory
from src.unsup.spectrum import eigen_cut as spectral_cut, estimate_keff
from src.unsup.dynamics import dis_check
from src.unsup.metrics import frobenius_relative, retrieval_mean_hungarian
from src.unsup.data import compute_round_coverage
from src.unsup.functions import propagate_J


__all__ = ["RoundLog", "single_round_step"]


@dataclass
class RoundLog:
    """Metriche aggregate del round."""
    retrieval: float
    fro: float
    keff: int
    coverage: float


def single_round_step(
    ETA_t: np.ndarray,        # (L, M_c, N)
    labels_t: np.ndarray,     # (L, M_c)
    xi_true: np.ndarray,      # (K, N)
    J_star: np.ndarray,       # (N, N) - riferimento ideale per FRO
    xi_prev: Optional[np.ndarray],  # None al round 0, poi (S, N)
    hp: HyperParams,
) -> Tuple[np.ndarray, np.ndarray, RoundLog]:
    """
    Esegue un round completo in SINGLE-mode e restituisce:
      - xi_ref_new   : memoria aggiornata per round successivo
      - J_KS         : matrice server (post-propagation) del round
      - RoundLog     : metriche

    Note:
      - mp: passa M_eff del *round corrente* a estimate_keff.
      - fallback memoria: se xi_r ha meno di K candidati, conserva tutti.
    """
    if ETA_t.ndim != 3:
        raise ValueError("ETA_t atteso (L, M_c, N).")
    if labels_t.ndim != 2:
        raise ValueError("labels_t atteso (L, M_c).")

    # 1) stima unsup per round t
    J_unsup, M_eff = build_unsup_J_single(ETA_t, K=hp.K)

    # 2) blending con memoria ebraica precedente (se presente)
    J_rec = blend_with_memory(J_unsup, xi_prev=xi_prev, w=hp.w)

    # 3) propagazione pseudo-inversa
    J_KS = np.asarray(propagate_J(J_rec, J_real=-1, verbose=False, iters=hp.prop.iters), dtype=np.float32)

    # 4) cut spettrale
    _spec_out = spectral_cut(J_KS, tau=hp.spec.tau, return_info=True)
    if len(_spec_out) == 3:
        V, k_eff_cut, info_spec = _spec_out
    else:  # fallback (shouldn't happen with return_info=True but guard defensively)
        V, k_eff_cut = _spec_out
        info_spec = {"evals": None}

    # 5) disentangling + magnetizzazioni (robusto con fallback interno)
    xi_r, _m_vec = dis_check(
        V=V,
        K=hp.K,
        L=hp.L,
        J_rec=J_rec,
        JKS_iter=J_KS,
        xi_true=xi_true,
        tam=hp.tam,
        spec=hp.spec,
        show_progress=hp.use_tqdm,
    )

    # 6) metriche
    #    6a) retrieval (matching ungherese)
    # NOTE: BUGFIX (2025-09-04): in precedenza si usava xi_r.astype(int) che, per valori float in (-1,1),
    #       li troncava a 0 abbattendo gli overlap (~0.1 medio). Ora binarizziamo con segno in {+1,-1}.
    xi_r_bin = np.where(xi_r >= 0, 1, -1).astype(np.int8)
    retr = retrieval_mean_hungarian(xi_r_bin, xi_true.astype(int))
    #    6b) coverage su questo round
    cov = compute_round_coverage(labels_t, K=hp.K)
    #    6c) FRO vs J*
    fro = frobenius_relative(J_KS, J_star)
    #    6d) K_eff
    if hp.estimate_keff_method == "mp":
        K_eff, _, _ = estimate_keff(J_KS, method="mp", M_eff=M_eff)
    else:
        K_eff, _, _ = estimate_keff(J_KS, method="shuffle")

    # --- DIAGNOSTICA FACOLTATIVA ---
    # Abilita impostando una variabile di ambiente UNSUP_DEBUG=1 per evitare stampe rumorose di default.
    import os
    if os.environ.get("UNSUP_DEBUG", "0") == "1":
        evals = info_spec.get("evals")
        # prime 5 autovalori
        top_evals = np.array2string(evals[:5], precision=3) if evals is not None else "[]"
        print(f"[DEBUG single_round] k_spec={k_eff_cut} K_eff={K_eff} retr~{float(retr):.3f} fro={float(fro):.3f} top_eigs={top_evals}")

    # 7) aggiorna memoria xi_ref per round successivo
    if xi_r_bin.shape[0] >= hp.K:
        xi_ref_new = xi_r_bin[: hp.K]
    else:
        xi_ref_new = xi_r_bin

    return xi_ref_new, J_KS, RoundLog(retrieval=float(retr), fro=float(fro), keff=int(K_eff), coverage=float(cov))


# ---- spectrum.py ----
# src/unsup/spectrum.py
from __future__ import annotations
from typing import Tuple, Dict, Any

import numpy as np

from .functions import estimate_K_eff_from_J as _estimate_K_eff_from_J


__all__ = [
    "eigen_cut",
    "estimate_keff",
]


def _symmetrize(J: np.ndarray) -> np.ndarray:
    return 0.5 * (np.asarray(J, dtype=np.float32) + np.asarray(J, dtype=np.float32).T)


def eigen_cut(
    J: np.ndarray,
    tau: float = 0.5,
    return_info: bool = False,
) -> Tuple[np.ndarray, int] | Tuple[np.ndarray, int, Dict[str, Any]]:
    """
    Seleziona gli autovettori associati ad autovalori > tau (default 0.5),
    restituendoli come righe (k_eff, N), compatibile con `dis_check`.

    Parametri
    ---------
    J : (N, N)
        Matrice (leggermente asimmetrica tollerata; viene simmetrizzata).
    tau : float
        Soglia su autovalori reali.
    return_info : bool
        Se True, restituisce anche info diagnostiche (evals, mask).

    Returns
    -------
    V_sel : (k_eff, N)
        Autovettori selezionati (trasposti per coerenza con codice esistente).
    k_eff : int
        Numero di componenti selezionate.
    info : dict (opzionale)
        {'evals': evals_desc, 'keep_mask': mask_desc}
    """
    J_sym = _symmetrize(J)
    # Use symmetric eigendecomposition for speed and stability
    evals, evecs = np.linalg.eigh(J_sym)

    # Ordina per autovalore decrescente
    order = np.argsort(evals)[::-1]
    evals_desc = evals[order]
    evecs_desc = evecs[:, order]

    keep_mask = evals_desc > float(tau)
    V_sel = evecs_desc[:, keep_mask].T  # (k_eff, N)
    k_eff = int(V_sel.shape[0])

    if return_info:
        return V_sel, k_eff, {"evals": evals_desc, "keep_mask": keep_mask}
    return V_sel, k_eff


def estimate_keff(
    J: np.ndarray,
    method: str = "shuffle",
    **kwargs,
) -> Tuple[int, np.ndarray, dict]:
    """
    Wrapper “pass-through” per la stima di K_eff.

    Parametri
    ---------
    J : (N, N)
        Matrice (propagata o meno) su cui stimare K_eff.
    method : {'shuffle', 'mp'}
        Metodo sottostante.
    **kwargs :
        Parametri addizionali inoltrati a `functions.estimate_K_eff_from_J`,
        p.es. alpha, n_random, M_eff (necessario per 'mp'), data_var.

    Returns
    -------
    K_eff : int
    keep_mask : (N,) bool
        Maschera sugli autovalori ORDINATI in senso decrescente.
    info : dict
        Dizionario diagnostico dal metodo sottostante (evals, soglie, ecc.).
    """
    # La funzione sottostante esegue già l'ordinamento discendente degli autovalori.
    return _estimate_K_eff_from_J(np.asarray(J, dtype=np.float32), method=method, **kwargs)


